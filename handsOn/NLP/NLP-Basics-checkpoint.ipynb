{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP(Natural Language Processing)\n",
    "\n",
    "* NLP is the field of study with the ability of a computer to understand, analyze, manipulate and potentially generate human languages.\n",
    "* By human language, simply refering to any language used for everyday communication.\n",
    "* NLP Encompasses Many Topics\n",
    "    1. Sentiment Analysis\n",
    "    2. Topic Modeling\n",
    "    3. Text Classification\n",
    "    4. Sentence segmentation or part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'Counter',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyIteratorList',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RUS_PICKLE',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'ToktokTokenizer',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'absolute_import',\n",
       " 'accuracy',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'align',\n",
       " 'alignment_error_rate',\n",
       " 'aline',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'choose',\n",
       " 'chunk',\n",
       " 'class_types',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collections',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corenlp',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deque',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edit_distance',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'find',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'improved_close_quote_regex',\n",
       " 'improved_open_quote_regex',\n",
       " 'improved_punct_regex',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'inspect',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_function',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'py25',\n",
       " 'py26',\n",
       " 'py27',\n",
       " 'pydoc',\n",
       " 'python_2_unicode_compatible',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'repp',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'string_types',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'sys',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'text',\n",
       " 'text_type',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toktok',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'treetransforms',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'types',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'viterbi',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see what is there in nltk package\n",
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what can we do with NLTK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'herself', 'been', 'with', 'here', 'very', 'doesn', 'won']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is just an example on how to use nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[0:500:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Basics: Reading in text data and why do we need to clean the text?\n",
    "###### Read in semi-structured text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\nham\\tOk lar... Joking wif u oni...\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tU dun say so early hor... U c already then say...\\nham\\tNah I don't think he goes to usf, he lives around here though\\nspam\\tFreeMsg Hey there darling it's been 3 week's now and no word bac\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the raw text\n",
    "rawData = open('smsspamcollection/SMSSpamCollection').read()\n",
    "\n",
    "# Print the raw data\n",
    "rawData[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseData = rawData.replace('\\t', '\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " 'ham',\n",
       " 'Ok lar... Joking wif u oni...']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parseData[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = parseData[0::2]\n",
    "textList = parseData[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam', 'ham', 'ham']\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\"]\n"
     ]
    }
   ],
   "source": [
    "print(labelList[0:5])\n",
    "print(textList[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b1d89d599144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataFrameDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataFrameDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfullCorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFrameDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfullCorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7354\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7356\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7358\u001b[0m     \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   7400\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7402\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "dataFrameDict = {}\n",
    "dataFrameDict['label'] = labelList\n",
    "dataFrameDict['body_list'] = textList\n",
    "fullCorpus = pd.DataFrame(dataFrameDict)\n",
    "\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5575\n",
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(labelList))\n",
    "print(len(textList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'ham', 'ham', '']\n"
     ]
    }
   ],
   "source": [
    "print(labelList[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_list\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last entry doesnot contain any info... we can ignore this \n",
    "fullCorpus = pd.DataFrame({\n",
    "    'label': labelList[:-1],\n",
    "    'body_list': textList\n",
    "})\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"smsspamcollection/SMSSpamCollection\", sep=\"\\t\", header=None)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Basics: Exploring the dataset\n",
    "###### Read in text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullCorpus = pd.read_csv(\"smsspamcollection/SMSSpamCollection\", sep=\"\\t\", header=None)\n",
    "fullCorpus.columns = ['label', 'body_text']\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 5572 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "# What is the shape of dataset?\n",
    "print(\"Input data has {} rows and {} columns\".format(len(fullCorpus), len(fullCorpus.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5572 rows, 4825 are ham and 747 are spam\n"
     ]
    }
   ],
   "source": [
    "# How many spam/ham are there?\n",
    "print(\"Out of {} rows, {} are ham and {} are spam\".format(len(fullCorpus), \n",
    "                                                          len(fullCorpus[fullCorpus.label == 'ham']),\n",
    "                                                          len(fullCorpus[fullCorpus.label == 'spam' ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null in tabel: 0\n",
      "Number of null in text: 0\n"
     ]
    }
   ],
   "source": [
    "# How much missing data is there?\n",
    "print(\"Number of null in tabel: {}\".format(fullCorpus['label'].isnull().sum()))\n",
    "print(\"Number of null in text: {}\".format(fullCorpus['body_text'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Regular Expression(Regex)\n",
    "* Text string for describing a search pattern\n",
    "* Why are Regular Expression Useful?\n",
    "    1. Identify whitespace b/w words/tokens\n",
    "    2. Identify/creating delimiters or end-of-line escape characters\n",
    "    3. Removing punctuation or numbers from your text\n",
    "    4. Identifying some textual patterns you are interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 01-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Basics: Learning how to use regular expressions\n",
    "###### Using regular expression in python\n",
    "* Python's 're' package is the most commonly used regex resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re_test = 'This sentence is made up from strings to test 2 different regex methods'\n",
    "re_test_complex = 'This     sentence is made up     from strings to test 2       different regex methods'\n",
    "re_test_complex1 = 'This-sentence-is-made/up.from*strings$to>>>>>>test---2\"\"\"\"\"different~regex-methods'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Splitting a sentence into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'made',\n",
       " 'up',\n",
       " 'from',\n",
       " 'strings',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'made',\n",
       " 'up',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'from',\n",
       " 'strings',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', re_test_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'made',\n",
       " 'up',\n",
       " 'from',\n",
       " 'strings',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', re_test_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This-sentence-is-made/up.from*strings$to>>>>>>test---2\"\"\"\"\"different~regex-methods']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', re_test_complex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'made',\n",
       " 'up',\n",
       " 'from',\n",
       " 'strings',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W is non word\n",
    "re.split('\\W+', re_test_complex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'made',\n",
       " 'up',\n",
       " 'from',\n",
       " 'strings',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'made',\n",
       " 'up',\n",
       " 'from',\n",
       " 'strings',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This-sentence-is-made/up.from*strings$to>>>>>>test---2\"\"\"\"\"different~regex-methods']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\S+', re_test_complex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'made',\n",
       " 'up',\n",
       " 'from',\n",
       " 'strings',\n",
       " 'to',\n",
       " 'test',\n",
       " '2',\n",
       " 'different',\n",
       " 'regex',\n",
       " 'methods']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', re_test_complex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "* Useful methods for tokenizing\n",
    "    1. findall():\n",
    "            findall() will search for actual words while ignoring the things that seperate the words.\n",
    "    2. split():\n",
    "           split() will search for the characters that seperte the words while ignoring the actaul words                             themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 01-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Replacing a specific string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep8_test = 'I try to follow PEP8 guidelines'\n",
    "pep7_test = 'I try to follow PEP7 guidelines'\n",
    "peep8_test = 'I try to follow PEEP8 guidelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['try', 'to', 'follow', 'guidelines']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall('[a-z]+', pep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'PEP']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]+', pep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PEEP8']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]+[0-9]+', peep8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I try to follow PEP8 Python Styleguide guidelines'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', peep8_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Other examples of regex methods\n",
    "* re.search(), re.match(), re.fullmatch(), re.finditer(), re.escape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here about pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 01-09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Basics: Implementing a pipeline to clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pre-processing text data\n",
    "    Cleaning up the text data is necessary to highlight attributes that you're going to want your machine learning\n",
    "    systen to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:\n",
    "    \n",
    "    1. Remove Panctuation\n",
    "    2. Tokenization\n",
    "    3. Remove Stopwords\n",
    "    4. Lemmatize/Stem\n",
    "    \n",
    "    The first three steps are implemented in pretty much any text cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "\n",
    "data = pd.read_csv(\"smsspamcollection/SMSSpamCollection\", sep=\"\\t\", header=None)\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below sentences meaning same to us but for python they are different. The only difference b/w them is of\n",
    "# punctuation(.) which actually does not contain any semantic information. That's why we must remove punctuation \n",
    "# from text data since they do not have information associated with them.\n",
    "\"I like NLP.\" == \"I line NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>[G, o,  , u, n, t, i, l,  , j, u, r, o, n, g,  , p, o, i, n, t,  , c, r, a, z, y,  , A, v, a, i,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[O, k,  , l, a, r,  , J, o, k, i, n, g,  , w, i, f,  , u,  , o, n, i]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[F, r, e, e,  , e, n, t, r, y,  , i, n,  , 2,  , a,  , w, k, l, y,  , c, o, m, p,  , t, o,  , w,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U,  , d, u, n,  , s, a, y,  , s, o,  , e, a, r, l, y,  , h, o, r,  , U,  , c,  , a, l, r, e, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[N, a, h,  , I,  , d, o, n, t,  , t, h, i, n, k,  , h, e,  , g, o, e, s,  , t, o,  , u, s, f,  ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                       body_text_clean  \n",
       "0  [G, o,  , u, n, t, i, l,  , j, u, r, o, n, g,  , p, o, i, n, t,  , c, r, a, z, y,  , A, v, a, i,...  \n",
       "1                                [O, k,  , l, a, r,  , J, o, k, i, n, g,  , w, i, f,  , u,  , o, n, i]  \n",
       "2  [F, r, e, e,  , e, n, t, r, y,  , i, n,  , 2,  , a,  , w, k, l, y,  , c, o, m, p,  , t, o,  , w,...  \n",
       "3  [U,  , d, u, n,  , s, a, y,  , s, o,  , e, a, r, l, y,  , h, o, r,  , U,  , c,  , a, l, r, e, a,...  \n",
       "4  [N, a, h,  , I,  , d, o, n, t,  , t, h, i, n, k,  , h, e,  , g, o, e, s,  , t, o,  , u, s, f,  ,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    text_nopunctuation = [ char for char in text if char not in string.punctuation]\n",
    "    return text_nopunctuation\n",
    "\n",
    "data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                       body_text_clean  \n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...  \n",
       "1                                                                              Ok lar Joking wif u oni  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "3                                                          U dun say so early hor U c already then say  \n",
       "4                                          Nah I dont think he goes to usf he lives around here though  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    text_nopunctuation = \"\".join([ char for char in text if char not in string.punctuation])\n",
    "    return text_nopunctuation\n",
    "\n",
    "data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punctuation(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 01-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                   body_text_tokenized  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]  \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'NLP' == 'nlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 01-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 02-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "* Process of reducing inflected (or sometimes derived) words to their word stem or root.\n",
    "* Crudely chopping off the end of the word to leave only the base\n",
    "\n",
    "Why do we care?\n",
    "* Reduces the corpus of words the model is exposed to\n",
    "* Explicitly correlates words with similar meanings( Replace a group of similar words with a common word)\n",
    "\n",
    "What are some stemmers?\n",
    "    1. Porter Stemmer\n",
    "    2. Snowball Stemmer\n",
    "    3. Lancaster Stemmer\n",
    "    4. Regex-Based Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 02-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplemental Data Cleaning: Using Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test out Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARTIN_EXTENSIONS',\n",
       " 'NLTK_EXTENSIONS',\n",
       " 'ORIGINAL_ALGORITHM',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_apply_rule_list',\n",
       " '_contains_vowel',\n",
       " '_ends_cvc',\n",
       " '_ends_double_consonant',\n",
       " '_has_positive_measure',\n",
       " '_is_consonant',\n",
       " '_measure',\n",
       " '_replace_suffix',\n",
       " '_step1a',\n",
       " '_step1b',\n",
       " '_step1c',\n",
       " '_step2',\n",
       " '_step3',\n",
       " '_step4',\n",
       " '_step5a',\n",
       " '_step5b',\n",
       " 'mode',\n",
       " 'pool',\n",
       " 'stem',\n",
       " 'unicode_repr',\n",
       " 'vowels']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grow\n",
      "grow\n",
      "grow\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('grows'))\n",
    "print(ps.stem('growing'))\n",
    "print(ps.stem('grow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "rune\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('run'))\n",
    "print(ps.stem('runing'))\n",
    "print(ps.stem('runner'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read in raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0                                                                        Ok lar... Joking wif u oni...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                                    U dun say so early hor... U c already then say...  \n",
       "3                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "4  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "data = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "      <td>[freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0                                                                        Ok lar... Joking wif u oni...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                                    U dun say so early hor... U c already then say...   \n",
       "3                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "4  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "3                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "4  [freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # First remove punctuations(special characters which don't carry any information)\n",
    "    text = \"\".join([ word for word in text if word not in string.punctuation])\n",
    "    \n",
    "    # Tokenize text now\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "    # Now remove Stopwords from tokenized text...\n",
    "    text = [ word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "      <td>[freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...</td>\n",
       "      <td>[freemsg, hey, darl, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chg, send, 150...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0                                                                        Ok lar... Joking wif u oni...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                                    U dun say so early hor... U c already then say...   \n",
       "3                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "4  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "2                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "3                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "4  [freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...   \n",
       "\n",
       "                                                                                     body_text_stemmed  \n",
       "0                                                                         [ok, lar, joke, wif, u, oni]  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "2                                                        [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "3                                                   [nah, dont, think, goe, usf, live, around, though]  \n",
       "4  [freemsg, hey, darl, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chg, send, 150...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(tokenized_text):\n",
    "    text = [ ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 02-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "* Process of grouping together the inflacted forms of a word so they can be analyzed as a single term, identified by the word's lemma\n",
    "* Using vocabulary analysis of words aiming to remove inflactional endings to return the dictionary form of a word\n",
    "\n",
    "How is Lemmatizing different from Stemming?\n",
    "* The goal of both is to condense derived words into their base forms\n",
    "* Stemming is typically faster as it simply chops off the end of a word using heuristics, without any understanding of the context in which a word is used.\n",
    "* Lemmatizing is typically more accurate as it uses more informed analysis to create groups of words with similar meaning based on the context around the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 02-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplemental Data Cleaning: Using a Lemmatizer\n",
    "\n",
    "###### Test out WordNet lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " 'lemmatize',\n",
       " 'unicode_repr']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean\n",
      "mean\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('meanness'))\n",
    "print(ps.stem('meaning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanness\n",
      "meaning\n"
     ]
    }
   ],
   "source": [
    "print(wn.lemmatize('meanness'))\n",
    "print(wn.lemmatize('meaning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goos\n",
      "gees\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('goose'))\n",
    "print(ps.stem('geese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goose\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "print(wn.lemmatize('goose'))\n",
    "print(wn.lemmatize('geese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0                                                                        Ok lar... Joking wif u oni...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                                    U dun say so early hor... U c already then say...  \n",
       "3                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "4  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in raw test\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "data = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "      <td>[freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0                                                                        Ok lar... Joking wif u oni...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                                    U dun say so early hor... U c already then say...   \n",
       "3                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "4  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "3                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "4  [freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # First remove punctuations(special characters which don't carry any information)\n",
    "    text = \"\".join([ word for word in text if word not in string.punctuation])\n",
    "    \n",
    "    # Tokenize text now\n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "    # Now remove Stopwords from tokenized text...\n",
    "    text = [ word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "      <td>[freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...</td>\n",
       "      <td>[freemsg, hey, darling, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0                                                                        Ok lar... Joking wif u oni...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                                    U dun say so early hor... U c already then say...   \n",
       "3                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "4  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "2                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "3                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "4  [freemsg, hey, darling, 3, weeks, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send...   \n",
       "\n",
       "                                                                                  body_text_lemmatized  \n",
       "0                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "3                                                    [nah, dont, think, go, usf, life, around, though]  \n",
       "4  [freemsg, hey, darling, 3, week, word, back, id, like, fun, still, tb, ok, xxx, std, chgs, send,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [ wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 03-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing\n",
    "\n",
    "* Process of encoding text as integers to create feature vectors.\n",
    "* Feature vecotor: An n-dimensional vector of numerical features that represent some object.\n",
    "\n",
    "Pipeline:\n",
    "    1. Raw Text - Model can not distinguish words\n",
    "    2. Tokenize - Tell the model what to look at\n",
    "    3. Clean Text - Remove stopwords, punctuation, stemming etc.\n",
    "    4. Vectorize - Convert text data to numeric form\n",
    "    5. Machine Learning Algorith - fit/train model\n",
    "    6. Spam Filter - System to filter emails\n",
    "    \n",
    "Why do we care?\n",
    "* When looking at a word, python only sees a string of characters\n",
    "* Raw text needs to be converted to numbers so that Python and the algorithms used for machine learning can understand\n",
    "\n",
    "Different types of Vectorization:\n",
    "    1. Count Vectorization\n",
    "    2. N-grams\n",
    "    3. Term frequency - inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 03-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Raw Data: Count Vectorization\n",
    "\n",
    "Count Vectorization:\n",
    "    Creates a document-term matrix where the entry of each cell will be a count of the numbers of times that word occurred in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4  spam   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0                                                                        Ok lar... Joking wif u oni...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                                    U dun say so early hor... U c already then say...  \n",
       "3                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "4  FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in text\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create function to remove punctuation, tokenize, remove stopwords and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join( [ word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ ps.stem(word) for word in tokens if tokens not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5571, 8377)\n",
      "['', '0', '008704050406', '0089mi', '0121', '01223585236', '01223585334', '0125698789', '02', '020603', '0207', '02070836089', '02072069400', '02073162414', '02085076972', '020903', '021', '050703', '0578', '06', '060505', '061104', '07008009200', '07046744435', '07090201529', '07090298926', '07099833605', '071104', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '07753741225', '0776xxxxxxx', '07786200117', '077xxx', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '07821230901', '0784987', '0789xxxxxxx', '0794674629107880867867', '0796xxxxxx', '07973788240', '07xxxxxxxxx', '0800', '08000407165', '08000776320', '08000839402', '08000930705', '08000938767', '08001950382', '08002888812', '08002986030', '08002986906', '08002988890', '08006344447', '0808', '08081263000', '08081560665', '0825', '0844', '08448350055', '08448714184', '0845', '08450542832', '08452810071', '08452810073', '08452810075over18', '0870', '08700621170150p', '08701213186', '08701237397', '08701417012', '08701417012150p', '0870141701216', '087016248', '08701752560', '087018728737', '0870241182716', '08702490080', '08702840625', '08702840625comuk', '08704439680', '08704439680tsc', '08706091795', '0870737910216yr', '08707500020', '08707509020', '0870753331018', '08707808226', '08708034412', '08708800282', '08709222922', '08709501522', '0870k', '087104711148', '08712101358', '08712103738', '0871212025016', '08712300220', '087123002209am7pm', '08712317606', '08712400200', '08712400603', '08712402050', '08712402578', '08712402779', '08712402902', '08712402972', '08712404000', '08712405020', '08712405022', '08712460324', '08712460324nat', '08712466669', '0871277810710pmin', '0871277810810', '0871277810910pmin', '087143423992stop', '087147123779am7pm', '08714712379', '08714712388', '08714712394', '08714712412', '08714714011', '08714719523', '08715203028', '08715203649', '08715203652', '08715203656', '08715203677', '08715203685', '08715203694', '08715205273', '08715500022', '08715705022', '08717111821', '08717168528', '08717205546', '08717507382', '08717507711', '08717509990', '08717890890', '08717895698', '08717898035', '08718711108', '08718720201', '08718723815', '08718725756', '08718726270', '08718726270150gbpmtmsg18', '08718726970', '08718726971', '08718726978', '087187272008', '08718727868', '08718727870', '08718729755', '08718729758', '08718730555', '08718730666', '08718738001', '08718738002', '08718738034', '08719180219', '08719180248', '08719181259', '08719181503', '08719181513', '08719839835', '08719899217', '08719899229', '08719899230', '09041940223', '09050000301', '09050000332', '09050000460', '09050000555', '09050000878', '09050000928', '09050001295', '09050001808', '09050002311', '09050003091', '09050005321', '09050090044', '09050280520', '09053750005', '09056242159', '09057039994', '09058091854', '09058091870', '09058094454', '09058094455', '09058094507', '09058094565', '09058094583', '09058094594', '09058094597', '09058094599', '09058095107', '09058095201', '09058097189', '09058097218', '09058098002', '09058099801', '09061104276', '09061104283', '09061209465', '09061213237', '09061221061', '09061221066', '09061701444', '09061701461', '09061701851', '09061701939', '09061702893', '09061743386', '09061743806', '09061743810', '09061743811', '09061744553', '09061749602', '09061790121', '09061790125', '09061790126', '09063440451', '09063442151', '09063458130', '0906346330', '09064011000', '09064012103', '09064012160', '09064015307', '09064017295', '09064017305', '09064018838', '09064019014', '09064019788', '09065069120', '09065069154', '09065171142stopsms08', '09065171142stopsms08718727870150ppm', '09065174042', '09065394514', '09065394973', '09065989180', '09065989182', '09066350750', '09066358152', '09066358361', '09066361921', '09066362206', '09066362220', '09066362231', '09066364311', '09066364349', '09066364589', '09066368327', '09066368470', '09066368753', '09066380611', '09066382422', '09066612661', '09066649731from', '09066660100', '09071512432', '09071512433', '09071517866', '09077818151', '09090204448', '09090900040', '09094100151', '09094646631', '09094646899', '09095350301', '09096102316', '09099725823', '09099726395', '09099726429', '09099726481', '09099726553', '09111030116', '09111032124', '09701213186', '0anetwork', '1', '10', '100', '1000', '10000', '100000', '1000call', '100603', '100psm', '1010', '1013', '101mega', '1030', '10803', '10am', '10am7pm', '10am9pm', '10k', '10p', '10pmin', '10ppm', '10th', '11', '1120', '113', '1131', '11414', '1146', '1148', '116', '1172', '118pmsg', '11mth', '12', '120', '12000pe', '1205', '121', '1225', '123', '1230', '125', '1250', '125gift', '128', '12hour', '12hr', '12mth', '12price', '13', '130', '131004', '1327', '13404', '139', '140', '1405', '140ppm', '145', '1450', '146tf150p', '14thmarch', '150', '1500', '150ea', '150morefrmmob', '150msg', '150mtmsgrcvd18', '150p', '150pday', '150perweeksub', '150perwksub', '150pm', '150pmeg', '150pmin', '150pmmorefrommobile2bremovedmobypobox734ls27yf', '150pmsg', '150pmsgrcvd', '150pmsgrcvdhgsuite3422landsroww1j6hl', '150pmt', '150pmtmsg', '150pmtmsgrcvd18', '150ppermesssubscript', '150ppm', '150ppmpobox10183bhamb64x', '150ppmsg', '150prcvd', '150psm', '150ptext', '150ptone', '150pw', '150pwk', '150rcvd', '150week', '150wk', '151', '1526', '153', '15541', '15pmin', '16', '1680', '169', '16onli', '177', '18', '180', '181104', '1843', '186', '18onli', '18ptxt', '18yr', '195', '1956669', '1U', '1appledayno', '1childish', '1cup', '1da', '1er', '1hanuman', '1hi', '1hr', '1im', '1lemondayno', '1mcflyall', '1million', '1minmobsmor', '1minmobsmorelkpobox177hp51fl', '1minmoremobsemspobox45po139wa', '1month', '1pm', '1s', '1st', '1st4term', '1stchoicecouk', '1stone', '1tulsi', '1u', '1unbreak', '1winaweek', '1winawk', '1x150pwk', '1yf', '2', '20', '200', '2000', '20000', '2003', '2004', '2005', '2006', '2007', '2025050', '20f', '20m12aq', '20p', '20pmin', '21', '211104', '215', '21870000hi', '21m', '21st', '22', '220cm2', '23', '2309', '230ish', '24', '241', '241004', '247mp', '24hr', '24m', '24th', '25', '250', '250k', '255', '25f', '25p', '260305', '261004', '261104', '2667', '26th', '2703', '27603', '28', '2814032', '285', '28day', '28th', '28thfebtc', '290305', '29100', '29m', '2B', '2C', '2I', '2U', '2bajarangabali', '2bold', '2channel', '2day', '2daylov', '2docdpleas', '2end', '2exit', '2ez', '2getha', '2geva', '2go', '2godid', '2gthr', '2hook', '2hr', '2im', '2kbsubject', '2marrow', '2moro', '2morow', '2morro', '2morrow', '2morrowxxxx', '2mro', '2mrw', '2mwen', '2naughti', '2nd', '2nhite', '2night', '2nite', '2nitetel', '2optout', '2optoutd3wv', '2p', '2polic', '2px', '2rcv', '2stop', '2stoptx', '2stoptxt', '2u', '2u2', '2untam', '2watershd', '2waxsto', '2when', '2wk', '2wt', '2wu', '2year', '2yr', '3', '30', '300', '3000', '300603', '300603tcsbcm4235wc1n3xxcallcost150ppmmobilesvari', '300p', '3030', '30apr', '30pptxt', '30th', '31', '3100', '310303', '311004', '31pmsg150p', '32000', '3230', '32323', '326', '32f', '330', '3350', '3365', '350', '3510i', '35p', '3650', '36504', '3680', '3680offer', '373', '3750', '375max', '38', '391784', '399', '3G', '3U', '3aj', '3cover', '3d', '3day', '3db', '3g', '3gbp', '3hr', '3lion', '3lp', '3maruti', '3mile', '3min', '3mobil', '3optic', '3pound', '3qxj9', '3rd', '3sentiment', '3ss', '3u', '3unkempt', '3uz', '3wife', '3wk', '3x', '3xx', '4', '40', '400', '400minscal', '402', '4041', '40411', '40533', '40gb', '40mph', '415', '41685', '41782', '420', '42049', '4217', '42478', '42810', '430', '434', '44', '4403ldnw1a7rw18', '447797706009', '447801259231', '447per', '448712404000pleas', '449050000301', '449071512431', '449month', '45', '450', '450p', '450ppw', '450pw', '45239', '46', '47', '4712', '4742', '48', '4882', '48922', '49557', '4U', '4a', '4brekki', '4cook', '4d', '4eva', '4few', '4fil', '4get', '4give', '4got', '4goten', '4info', '4jx', '4lux', '4mi', '4mth', '4o', '4pavanaputra', '4press', '4rowdi', '4some1', '4tctxt', '4th', '4the', '4thnovbehind', '4txt120p', '4txt120', '4u', '4ui', '4utxt', '4w', '4ward', '4wrd', '4year', '5', '50', '500', '5000', '500000', '505060', '50award', '50p', '515', '515pm', '5226', '5249', '526', '528', '530', '532', '54', '542', '545', '5903', '5I', '5K', '5digit', '5free', '5ful', '5garden', '5gentli', '5ish', '5min', '5ml', '5month', '5p', '5pm', '5sankatmochan', '5terror', '5th', '5wb', '5we', '5wkg', '5wq', '5year', '6', '600', '6031', '60400thousadi', '60p', '60pmin', '61200', '61610', '62220cncl', '6230', '62468', '62735', '630', '63mile', '645', '645pm', '650', '6669', '67441233', '68866', '69101', '69200', '69669', '69696', '69698', '69855', '6986618', '69876', '69888', '69888nyt', '69911', '69969', '69988', '6cruel', '6day', '6hl', '6housemaid', '6hr', '6ish', '6miss', '6month', '6pm', '6ramaduth', '6romant', '6th', '6time', '6wu', '6zf', '7', '700', '71', '725', '7250', '7250i', '730', '730ish', '730pm', '731', '74355', '750', '75000', '7548', '7634', '7684', '7732584351', '78', '786', '7876150ppm', '78pmin', '79', '7am', '7cfca1a', '7children', '7ish', '7mahav', '7oz', '7pm', '7romant', '7shi', '7th', '7w', '7z', '8', '80', '800', '8000930705', '80062', '8007', '80082', '80086', '80122300pwk', '80155', '80160', '80182', '8027', '80488', '80488biz', '80608', '8077', '80878', '81010', '81151', '81303', '81618', '816183', '82242', '82277', '82277unsub', '82324', '82468', '830', '83021', '83039', '83049', '83110', '83118', '83222', '83332pleas', '83338', '83355', '83370', '83383', '83435', '83600', '83738', '84', '84025', '84122', '84128', '84128custcar', '84199', '84484', '85', '850', '85023', '85069', '85222', '85233', '8552', '85555', '86021', '861', '863', '864233', '86688', '86888', '87021', '87066', '87070', '87077', '87121', '87131', '8714714', '87239', '87575', '8800', '88039', '88039skilgmetscs087147403231winawkage16', '88066', '88088', '88222', '8830', '88600', '88800', '8883', '88877', '88877free', '88888', '89034', '89070', '89080', '89105', '89123', '89545', '89555', '89693', '89938', '8am', '8attract', '8ball', '8hr', '8lb', '8lovabl', '8neighbour', '8o', '8pm', '8th', '8wp', '9', '900', '9061100010', '9153', '924', '92h', '930', '945', '946', '95pax', '96', '97n7qp', '98321561', '9996', '9ae', '9am', '9am11pm', '9decent', '9funni', '9ja', '9pm', '9t', '9th', '9yt', 'A', 'AD', 'AG', 'AH', 'AL', 'AM', 'AN', 'AS', 'AT', 'AV', 'Ab', 'Ah', 'Al', 'Am', 'An', 'As', 'At', 'Ay', 'B', 'B4', 'BE', 'BK', 'BT', 'BY', 'Bc', 'Be', 'Bt', 'Bx', 'By', 'C', 'CC', 'CD', 'CL', 'CM', 'CU', 'Co', 'Cs', 'D', 'DA', 'DD', 'DE', 'DO', 'Da', 'De', 'Do', 'Dr', 'E', 'ER', 'EY', 'Ee', 'Eh', 'Em', 'En', 'Er', 'Ew', 'F', 'FA', 'Fr', 'G', 'G2', 'GB', 'GO', 'Gd', 'Ge', 'Gn', 'Go', 'H', 'HI', 'HL', 'HM', 'HU', 'Ha', 'He', 'Hi', 'Hm', 'Ho', 'I', 'ID', 'IF', 'IL', 'IM', 'IN', 'IQ', 'IS', 'IT', 'Ic', 'Id', 'If', 'Im', 'In', 'Is', 'It', 'J', 'JD', 'K', 'KR', 'Ki', 'Ku', 'L', 'LE', 'Lk', 'M', 'M6', 'ME', 'MF', 'MO', 'MR', 'MY', 'Ma', 'Me', 'Mm', 'Mr', 'My', 'N', 'NO', 'No', 'Nt', 'Nw', 'O', 'O2', 'OF', 'OH', 'OK', 'ON', 'OR', 'Of', 'Oh', 'Oi', 'Ok', 'On', 'Or', 'Oz', 'P', 'PA', 'PC', 'PO', 'PS', 'Pa', 'Pg', 'Pl', 'Po', 'Q', 'R', 'RV', 'Re', 'Rs', 'S', 'S8', 'SD', 'SF', 'SI', 'SN', 'SO', 'SP', 'ST', 'Si', 'So', 'St', 'T', 'TA', 'TC', 'TH', 'TO', 'TV', 'TX', 'Ta', 'Tb', 'To', 'Ts', 'U', 'U4', 'UK', 'UP', 'UR', 'US', 'UU', 'Uh', 'Up', 'Ur', 'Us', 'V', 'VE', 'VU', 'W4', 'WE', 'WK', 'Wa', 'We', 'Wk', 'Wn', 'X', 'X2', 'XX', 'Xx', 'Xy', 'Y', 'YA', 'YM', 'YO', 'Ya', 'Yo', 'Z', 'a', 'a21', 'a30', 'aa', 'aah', 'aaniy', 'aaooooright', 'aathilov', 'aathiwher', 'abbey', 'abdomen', 'abeg', 'abelu', 'aberdeen', 'abi', 'abil', 'abiola', 'abj', 'abl', 'abnorm', 'about', 'abouta', 'abov', 'abroad', 'absenc', 'absolut', 'abstract', 'abt', 'abta', 'aburo', 'abus', 'ac', 'academ', 'acc', 'accent', 'accentur', 'accept', 'access', 'accid', 'accident', 'accommod', 'accommodationvouch', 'accomod', 'accordin', 'accordingli', 'accordinglyor', 'account', 'accumul', 'ach', 'achanammarakheshqatar', 'achiev', 'acid', 'acknowledg', 'acl03530150pm', 'acnt', 'acoentry41', 'across', 'acsmsreward', 'act', 'actin', 'action', 'activ', 'activ8', 'actor', 'actual', 'acwicmb3cktz8r74', 'ad', 'adam', 'add', 'addamsfa', 'addi', 'addict', 'address', 'addressul', 'adewal', 'adi', 'adjust', 'admin', 'administr', 'admir', 'admiss', 'admit', 'admiti', 'ador', 'adp', 'adress', 'adrian', 'adrink', 'adsens', 'adult', 'advanc', 'adventur', 'advic', 'advis', 'advisor', 'aeronaut', 'aeroplan', 'afew', 'affair', 'affect', 'affection', 'affectionsamp', 'affidavit', 'afford', 'afghanistan', 'afraid', 'africa', 'african', 'aft', 'after', 'afternon', 'afternoon', 'afterward', 'aftr', 'again', 'againcal', 'againlov', 'against', 'agalla', 'age', 'age16', 'age16150ppermesssubscript', 'age23', 'agenc', 'agent', 'agesr', 'agidhan', 'ago', 'agocusoon', 'agre', 'agreen', 'ah', 'aha', 'ahead', 'ahge', 'ahhh', 'ahhhhjust', 'ahmad', 'ahnow', 'ahold', 'ahsen', 'ahth', 'ahwhat', 'aid', 'aig', 'aight', 'aint', 'air', 'air1', 'airport', 'airtel', 'aiya', 'aiyah', 'aiyar', 'aiyo', 'ajith', 'ak', 'aka', 'akonlon', 'al', 'alaikkumprid', 'alaipayuth', 'albi', 'album', 'albumquit', 'alcohol', 'aldrin', 'alert', 'alertfrom', 'alett', 'alex', 'alfi', 'algarv', 'algebra', 'algorithm', 'ali', 'alian', 'alibi', 'aliv', 'alivebett', 'all', 'allah', 'allahmeet', 'allahrakhesh', 'allalo', 'allday', 'allo', 'allow', 'almost', 'alon', 'along', 'alot', 'alreadi', 'alreadysabarish', 'alright', 'alrightokay', 'alrit', 'alritehav', 'also', 'alsoor', 'alter', 'alternativehop', 'although', 'alwa', 'alway', 'alwi', 'am', 'amanda', 'amaz', 'ambiti', 'ambrithmaduraimet', 'american', 'ami', 'amigo', 'amk', 'ammaelif', 'ammo', 'amnow', 'among', 'amongst', 'amount', 'amp', 'amplikat', 'amrca', 'amrita', 'amt', 'amus', 'amx', 'an', 'ana', 'anal', 'analysi', 'anand', 'and', 'anderson', 'andor', 'andr', 'andrewsboy', 'andro', 'angel', 'angri', 'ani', 'anim', 'anji', 'anjola', 'anna', 'anni', 'anniversari', 'annonc', 'announc', 'annoy', 'annoyin', 'anonym', 'anot', 'anoth', 'ansr', 'answer', 'answerin', 'answr', 'antelop', 'anthoni', 'anti', 'antibiot', 'anybodi', 'anyhow', 'anymor', 'anyon', 'anyplac', 'anyth', 'anythi', 'anythin', 'anythingtomorrow', 'anytim', 'anyway', 'anywher', 'aom', 'apart', 'ape', 'apeshit', 'aphex', 'apnt', 'apo', 'apolog', 'apologet', 'apologis', 'app', 'appar', 'appeal', 'appear', 'appendix', 'appi', 'applebe', 'applespairsal', 'appli', 'applic', 'apply2', 'appoint', 'appreci', 'approach', 'appropri', 'approv', 'approx', 'appt', 'april', 'aproach', 'apt', 'aptitud', 'aquariu', 'ar', 'arab', 'arabian', 'arcad', 'archiv', 'ard', 'ard', 'are', 'area', 'arent', 'arestaur', 'aretak', 'argentina', 'argh', 'argu', 'argument', 'ari', 'aris', 'arithmet', 'arm', 'armand', 'armenia', 'arng', 'arngd', 'arnt', 'around', 'aroundn', 'arpraveesh', 'arr', 'arrang', 'arrest', 'arriv', 'arrow', 'arsen', 'art', 'arti', 'artist', 'arul', 'arun', 'as', 'asa', 'asap', 'asapok', 'asda', 'ash', 'ashley', 'ashwini', 'asia', 'asian', 'ask', 'askd', 'askin', 'aslamalaikkuminsha', 'asleep', 'aspect', 'ass', 'assess', 'asshol', 'assist', 'associ', 'assum', 'asther', 'asthma', 'astn', 'astoundingli', 'astrolog', 'astronom', 'asu', 'asusual1', 'at', 'ate', 'athlet', 'athom', 'atlanta', 'atlast', 'atleast', 'atm', 'atroci', 'attach', 'attack', 'attempt', 'atten', 'attend', 'attent', 'attitud', 'attract', 'attractioni', 'attribut', 'atyour', 'auction', 'auctionpunj', 'audiit', 'audit', 'audrey', 'audri', 'august', 'aunt', 'aunti', 'aust', 'australia', 'authoris', 'auto', 'autocorrect', 'av', 'ava', 'avail', 'availa', 'availablei', 'availablethey', 'avalarr', 'avatar', 'avbl', 'ave', 'aveng', 'avent', 'avenu', 'avin', 'avo', 'avoid', 'await', 'awak', 'award', 'away', 'awesom', 'awkward', 'aww', 'awww', 'ax', 'axi', 'ayn', 'ayo', 'b', 'b4', 'b4190604', 'b4280703', 'b4u', 'ba', 'ba128nnfwfly150ppm', 'baaaaaaaab', 'baaaaab', 'babe', 'babeprobpop', 'babesozi', 'babi', 'babygoodby', 'babyhop', 'babyjontet', 'babysit', 'bac', 'back', 'backa', 'backdoor', 'backward', 'bad', 'badass', 'badli', 'badrith', 'bag', 'bagi', 'bahama', 'baig', 'bailiff', 'bak', 'bakra', 'bakrid', 'balanc', 'ball', 'baller', 'balloon', 'bam', 'bambl', 'ban', 'band', 'bandag', 'bang', 'bangb', 'bangbab', 'bani', 'bank', 'banneduk', 'banter', 'bao', 'bar', 'barbi', 'barcelona', 'bare', 'bari', 'barkley', 'barm', 'barolla', 'barrel', 'barri', 'base', 'bash', 'basic', 'basket', 'basketbal', 'basqihav', 'bat', 'batch', 'batchlor', 'bath', 'bathroom', 'batsman', 'batt', 'batteri', 'battl', 'bawl', 'bay', 'bb', 'bbc', 'bbdelux', 'bbdpooja', 'bbdtht', 'bblue', 'bbq', 'bc', 'bcaz', 'bck', 'bcm', 'bcm1896wc1n3xx', 'bcm4284', 'bcmsfwc1n3xx', 'bcoz', 'bcozi', 'bcum', 'bcz', 'bday', 'be', 'beach', 'bead', 'bear', 'beat', 'beauti', 'beautifulmay', 'bec', 'becau', 'becaus', 'becausethey', 'becom', 'becoz', 'becz', 'bed', 'bedbut', 'bedreal', 'bedrm', 'bedrm900', 'bedroom', 'bedroomlov', 'beeen', 'beehoon', 'been', 'beendrop', 'beer', 'beerag', 'beerr', 'befor', 'beforehand', 'beforew', 'beg', 'beggar', 'begin', 'begun', 'behalf', 'behav', 'behind', 'bein', 'believ', 'beliv', 'bell', 'bellearli', 'belli', 'belliger', 'belong', 'belov', 'belovd', 'belt', 'ben', 'bend', 'beneath', 'beneficiari', 'benefit', 'benni', 'bergkamp', 'besid', 'best', 'best1', 'bestcongrat', 'bestrpli', 'bet', 'beta', 'beth', 'betta', 'better', 'bettersn', 'between', 'beverag', 'bevieswaz', 'bewar', 'beyond', 'bf', 'bff', 'bfore', 'bhaskar', 'bhayandar', 'bian', 'biatch', 'bid', 'big', 'bigger', 'biggest', 'bike', 'bill', 'billi', 'billion', 'bilo', 'bimbo', 'bin', 'biola', 'bird', 'birla', 'biro', 'birth', 'birthdat', 'birthday', 'bishan', 'bit', 'bitch', 'bite', 'bk', 'black', 'blackand', 'blackberri', 'blackim', 'blacko', 'blah', 'blake', 'blame', 'blank', 'blanket', 'blastin', 'bleak', 'bleh', 'bless', 'blessget', 'blimey', 'blind', 'block', 'blog', 'bloke', 'blond', 'bloo', 'blood', 'bloodblood', 'bloodi', 'bloodsend', 'bloomberg', 'bloombergcom', 'blow', 'blown', 'blu', 'blue', 'bluetooth', 'bluetoothhdset', 'blueu', 'bluff', 'blur', 'bluray', 'bmw', 'board', 'boat', 'boatin', 'bob', 'bodi', 'boggi', 'bognor', 'bold', 'bold2', 'bollox', 'boltblu', 'bomb', 'bone', 'bong', 'bonu', 'boo', 'boob', 'book', 'bookedth', 'bookmark', 'bookshelf', 'boooo', 'boost', 'booti', 'bootydeli', 'borderlin', 'bore', 'borin', 'born', 'bornpleas', 'borrow', 'boss', 'boston', 'bot', 'both', 'bother', 'bottl', 'bottom', 'bought', 'boundari', 'bout', 'boutxx', 'bowa', 'bowl', 'box', 'box1146', 'box139', 'box177', 'box245c2150pm', 'box326', 'box334', 'box334sk38ch', 'box385', 'box39822', 'box403', 'box420', 'box42wr29c', 'box434sk38wp150ppm18', 'box61m60', 'box95qu', 'box97n7qp', 'boy', 'boyf', 'boyfriend', 'boyi', 'boytoy', 'bpo', 'bra', 'brah', 'brain', 'braindanc', 'braini', 'brainless', 'brand', 'brandi', 'brat', 'brave', 'bray', 'brb', 'brdget', 'bread', 'breadstick', 'break', 'breaker', 'breakfast', 'breakin', 'breath', 'breathe1', 'breather', 'breez', 'breezi', 'bribe', 'bridg', 'bridgwat', 'brief', 'bright', 'brighten', 'brilliant', 'brilliant1thingi', 'brilliantli', 'brin', 'bring', 'brisk', 'brison', 'bristol', 'british', 'britney', 'bro', 'broad', 'broadband', 'broke', 'broken', 'brolli', 'broth', 'brotha', 'brother', 'brought', 'browni', 'brows', 'browser', 'browsin', 'bruce', 'brum', 'bruv', 'bslvyl', 'bsn', 'bsnl', 'bstfrnd', 'bt', 'bthere', 'bthmm', 'btnation', 'btnationalr', 'btooth', 'btw', 'btwn', 'bu', 'buck', 'bud', 'buddi', 'budget', 'buen', 'buff', 'buffet', 'buffi', 'bugi', 'build', 'built', 'bulb', 'bull', 'bullshit', 'bun', 'bunch', 'bundl', 'bunker', 'burden', 'burger', 'burgundi', 'burial', 'burn', 'burnt', 'burrito', 'bus822656166382', 'buse', 'busetop', 'busi', 'busti', 'busyi', 'but', 'butt', 'butther', 'button', 'buy', 'buyer', 'buz', 'buzi', 'buzz', 'buzzzz', 'bw', 'bx420', 'bx420ip45w', 'bx526', 'by', 'byatch', 'bye', 'c', 'c52', 'cab', 'cabin', 'cabl', 'cafe', 'cage', 'cake', 'caken', 'cal', 'calcul', 'cali', 'calicut', 'california', 'call', 'call09050000327', 'call2optout4qf2', 'call2optout674', 'call2optoutf4q', 'call2optouthf8', 'call2optoutj', 'call2optoutj5q', 'call2optoutlf56', 'call2optoutn9dx', 'call2optoutyhl', 'callback', 'callcost', 'callcoz', 'calld', 'calldrov', 'caller', 'callertun', 'callfreefon', 'callin', 'callingforgot', 'callon', 'calls150ppm', 'callsmessagesmiss', 'callurg', 'calm', 'cam', 'camcord', 'came', 'camera', 'cameravideo', 'camp', 'campu', 'camri', 'can', 'canada', 'canal', 'canari', 'cancel', 'cancer', 'candont', 'canlov', 'cannam', 'cannot', 'cannt', 'cant', 'cantdo', 'canteen', 'cap', 'capac', 'capit', 'cappuccino', 'captain', 'car', 'card', 'cardiff', 'cardin', 'care', 'careabout', 'career', 'careinsha', 'careless', 'carent', 'careswt', 'careumma', 'carewhoev', 'carli', 'carlin', 'carlo', 'carlosl', 'carolin', 'carolina', 'carpark', 'carri', 'carryin', 'carso', 'carton', 'cartoon', 'case', 'cash', 'cashbal', 'cashbincouk', 'cashin', 'cashto', 'cast', 'castor', 'casualti', 'cat', 'catch', 'categori', 'caught', 'caus', 'cave', 'caveboy', 'cbe', 'cc100pmin', 'ccna', 'cd', 'cdgt', 'cedar', 'ceil', 'celeb', 'celeb4', 'celebr', 'cell', 'censu', 'center', 'centr', 'centuri', 'cer', 'cereal', 'ceri', 'certainli', 'certif', 'cha', 'chachi', 'chad', 'chain', 'challeng', 'champ', 'champlaxig', 'champney', 'chanc', 'chang', 'channel', 'chap', 'chapel', 'chapter', 'charact', 'charg', 'charged150pmsg2', 'chariti', 'charl', 'charli', 'charm', 'chart', 'chase', 'chastiti', 'chat', 'chat80155', 'chatim', 'chatlin', 'chatter', 'cheap', 'cheaper', 'cheat', 'chechi', 'check', 'checkbox', 'checkin', 'checkmat', 'checkup', 'cheek', 'cheer', 'cheeri', 'chees', 'cheesi', 'cheeto', 'chef', 'chennai', 'chennaibecaus', 'chennaii', 'chequ', 'cherish', 'cherthalain', 'chess', 'chest', 'chex', 'cheyyamoand', 'chez', 'chg', 'chic', 'chick', 'chicken', 'chief', 'chik', 'chikku', 'chikkuali', 'chikkub', 'chikkudb', 'chikkugo', 'chikkuil', 'chikkuk', 'chikkusimpl', 'chikkuwat', 'child', 'childish', 'childporn', 'children', 'chile', 'chill', 'chillaxin', 'chillin', 'china', 'chinatown', 'chinchilla', 'chines', 'chinki', 'chiong', 'chip', 'chitchat', 'chk', 'chloe', 'chocol', 'choic', 'choos', 'chop', 'chord', 'chore', 'chosen', 'chrgd50p', 'christ', 'christian', 'christma', 'christmasmerri', 'christmassi', 'chuck', 'chuckin', 'church', 'ciao', 'cin', 'cine', 'cinema', 'citi', 'citizen', 'citylink', 'cla', 'claim', 'claimcod', 'clair', 'clarif', 'clarifi', 'clash', 'class', 'classic', 'classmat', 'claypot', 'cld', 'clean', 'clear', 'clearer', 'clearli', 'clever', 'click', 'cliff', 'clip', 'clock', 'clos1', 'close', 'closebi', 'closedinclud', 'closer', 'closingdate040902', 'cloth', 'cloud', 'clover', 'club', 'club4', 'club4mobilescom', 'clue', 'cm', 'cme', 'cmon', 'cn', 'cnl', 'cnn', 'co', 'coach', 'coast', 'coat', 'coax', 'cocacola', 'coccoon', 'cochin', 'cock', 'cocksuck', 'coco', 'code', 'code4xx26', 'coffe', 'coher', 'coimbator', 'coin', 'coincid', 'colani', 'cold', 'coldheard', 'colin', 'collag', 'collaps', 'colleagu', 'collect', 'colleg', 'collegexx', 'color', 'colour', 'colourredtextcolourtxtstar', 'com', 'comb', 'combin', 'come', 'comedi', 'comedyc', 'comei', 'cometil', 'comfey', 'comfort', 'comin', 'comingdown', 'comingtmorow', 'command', 'comment', 'commerci', 'commit', 'common', 'commun', 'comp', 'compani', 'companion', 'compar', 'compass', 'compens', 'competit', 'complac', 'complain', 'complaint', 'complementari', 'complet', 'complex', 'compliment', 'complimentari', 'compofstuff', 'comprehens', 'compromis', 'compulsori', 'comput', 'computerless', 'comuk220cm2', 'con', 'conact', 'concentr', 'concern', 'concert', 'conclus', 'condit', 'conditionand', 'conduct', 'conect', 'confer', 'confid', 'configur', 'confirm', 'confirmd', 'confirmdeni', 'conform', 'confus', 'congrat', 'congratul', 'connect', 'consensu', 'consent', 'conserv', 'consid', 'consist', 'consol', 'constant', 'constantli', 'contact', 'contain', 'content', 'contin', 'continu', 'contract', 'contribut', 'control', 'conveni', 'convers', 'convert', 'convey', 'convinc', 'convincingjust', 'cook', 'cooki', 'cool', 'coolmob', 'coop', 'cooper', 'cop', 'cope', 'copi', 'corect', 'cornwal', 'corpor', 'corrct', 'correct', 'correctionor', 'correctli', 'corrupt', 'corvett', 'cosign', 'cost', 'costa', 'costum', 'couch', 'cougarpen', 'cough', 'could', 'coulda', 'couldn', 'couldnt', 'count', 'countin', 'countinlot', 'countri', 'coupl', 'coupla', 'courag', 'cours', 'court', 'courtroom', 'cousin', 'cover', 'coveragd', 'coz', 'cozi', 'cozsomtim', 'cp', 'cr', 'cr01327bt', 'cr9', 'crab', 'crack', 'craigslist', 'cram', 'cramp', 'crap', 'crash', 'crave', 'crazi', 'craziest', 'crazyin', 'cream', 'creat', 'creativ', 'cred', 'credit', 'creep', 'creepi', 'cresubi', 'cri', 'cribb', 'cricket', 'crickit', 'crisi', 'crisisspk', 'cro1327', 'crore', 'cross', 'crowd', 'croydon', 'crucial', 'crucifi', 'cruis', 'cruisin', 'crush', 'cs', 'csh11', 'cst', 'cstore', 'ctagg', 'ctargg', 'cthen', 'ctla', 'cttargg', 'ctter', 'cttergg', 'cuck', 'cud', 'cuddl', 'cudnt', 'culdnt', 'cultur', 'cum', 'cumin', 'cup', 'cupboard', 'cuppa', 'curfew', 'curiou', 'current', 'curri', 'curtsey', 'cust', 'custcar', 'custcare08718720201', 'custom', 'customercar', 'customersqueriesnetvisionukcom', 'cut', 'cute', 'cutefrnd', 'cutest', 'cuti', 'cutter', 'cuz', 'cw25wx', 'cya', 'cyclist', 'cyst', 'd', 'da', 'daal', 'daalway', 'dabbl', 'dabook', 'dad', 'daddi', 'dado', 'dagood', 'dahe', 'dahow', 'dai', 'daili', 'dajst', 'dammit', 'damn', 'dan', 'danalla', 'danc', 'dancc', 'dancin', 'dane', 'dang', 'danger', 'dao', 'dapleas', 'dare', 'dark', 'darker', 'darkest', 'darl', 'darlin', 'darlinim', 'darren', 'dartboard', 'dasara', 'dat', 'data', 'date', 'datebox1282essexcm61xn', 'datingi', 'datoday', 'datz', 'daurgent', 'dave', 'dawhat', 'dawher', 'dawn', 'day', 'day2', 'day2find', 'dayexcept', 'dayha', 'daysh', 'daysso', 'dayswil', 'daysn', 'daytim', 'dayu', 'daywith', 'de', 'dead', 'deadwel', 'deal', 'dealer', 'dealfarm', 'deam', 'dear', 'dear1', 'dearer', 'deari', 'dearli', 'dearlov', 'dearm', 'dearrakhesh', 'dearregret', 'dearshal', 'dearslp', 'deartak', 'death', 'debat', 'dec', 'decad', 'decemb', 'decid', 'decim', 'decis', 'deck', 'declar', 'decor', 'dedic', 'deduct', 'deep', 'deepak', 'deepest', 'deer', 'deeraj', 'def', 'defeat', 'defer', 'definit', 'definitli', 'defo', 'degre', 'dehydr', 'del', 'delay', 'delet', 'delhi', 'delici', 'deliv', 'deliveredtomorrow', 'deliveri', 'deltomorrow', 'delux', 'dem', 'demand', 'den', 'dena', 'dengra', 'deni', 'dent', 'dental', 'dentist', 'depart', 'depend', 'deposit', 'depress', 'dept', 'der', 'derek', 'derp', 'describ', 'descript', 'desert', 'deserv', 'design', 'desir', 'desk', 'despar', 'desper', 'despit', 'dessert', 'destin', 'destini', 'detail', 'detailsi', 'determin', 'detroit', 'deu', 'develop', 'devic', 'devil', 'devour', 'dey', 'deyhop', 'deyi', 'dha', 'dhina', 'dhoni', 'dhort', 'di', 'dial', 'diall', 'dialogu', 'diamond', 'diaper', 'dice', 'dick', 'dict', 'dictionari', 'did', 'diddi', 'didn', 'didnt', 'didntgiv', 'didt', 'die', 'diesel', 'diet', 'diff', 'differ', 'differb', 'difficult', 'difficulti', 'dificult', 'digi', 'digit', 'digniti', 'dileepthank', 'dime', 'dimens', 'din', 'dine', 'dinero', 'ding', 'dinner', 'dinnermsg', 'dino', 'dint', 'dip', 'dippeditinadew', 'direct', 'directli', 'director', 'dirt', 'dirti', 'dirtiest', 'disagre', 'disappear', 'disappoint', 'disast', 'disastr', 'disc', 'disclos', 'disconnect', 'discount', 'discreet', 'discuss', 'diseas', 'diskyou', 'dislik', 'dismay', 'dismissi', 'display', 'distanc', 'distract', 'disturb', 'disturbancemight', 'ditto', 'divert', 'divis', 'divorc', 'diwali', 'dizzamn', 'dizze', 'dl', 'dled', 'dlf', 'dload', 'dnt', 'do', 'dob', 'dobbi', 'doc', 'dock', 'doctor', 'document', 'dodda', 'dodgey', 'doe', 'doesdiscountshitinnit', 'doesn', 'doesnt', 'dog', 'dogbreath', 'dogg', 'doggi', 'doggin', 'dogwood', 'doin', 'doinat', 'doinghow', 'doingwhat', 'doinnearli', 'dointerest', 'doke', 'dokey', 'doll', 'dollar', 'dolld', 'dom', 'domain', 'don', 'donat', 'done', 'donew', 'donno', 'dont', 'dont4get2text', 'dontcha', 'dontignor', 'dontpleas', 'donyt', 'doom', 'door', 'dorm', 'dormitori', 'dorothykiefercom', 'dose', 'dosometh', 'dot', 'doubl', 'doublefaggot', 'doublemin', 'doubletxt', 'doubt', 'doug', 'dough', 'down', 'download', 'downon', 'downstem', 'dozen', 'dp', 'dr', 'dracula', 'drama', 'dramastorm', 'dramat', 'drastic', 'draw', 'drawpleas', 'dread', 'dream', 'dreamlov', 'dreamsmuah', 'dreamstak', 'dreamsu', 'dreamz', 'dress', 'dresser', 'dri', 'drink', 'drinkin', 'drinkpa', 'drive', 'driver', 'drivin', 'drizzl', 'drm', 'drmstake', 'drop', 'drove', 'drpd', 'drug', 'drugdeal', 'drum', 'drunk', 'drunkard', 'drunken', 'drvgsto', 'dryer', 'dsnt', 'dt', 'dual', 'dub', 'dubsack', 'duchess', 'duck', 'dude', 'dudett', 'due', 'duffer', 'dull', 'dumb', 'dump', 'dun', 'dungere', 'dunno', 'duo', 'durban', 'dure', 'durham', 'dusk', 'dust', 'duvet', 'dvd', 'dvg', 'dwn', 'dysentri', 'e', 'e14', 'each', 'eachoth', 'ear', 'earli', 'earlier', 'earlierw', 'earliest', 'earn', 'earth', 'earthsofa', 'easi', 'easier', 'easiest', 'easili', 'east', 'eastend', 'easter', 'eat', 'eaten', 'eatin', 'ebay', 'ec2a', 'echo', 'eckankar', 'ecstaci', 'ecstasi', 'edg', 'edha', 'edison', 'edit', 'edrunk', 'educ', 'edukkukaye', 'edward', 'ee', 'eek', 'eeri', 'eerulli', 'effect', 'effici', 'efreefon', 'eg', 'eg23f', 'eg23g', 'egbon', 'egg', 'eggpotato', 'eggspert', 'ego', 'eh', 'eh74rr', 'eight', 'eighth', 'eightish', 'eir', 'either', 'el', 'ela', 'elabor', 'elain', 'elama', 'elaya', 'eldest', 'elect', 'electr', 'eleph', 'eleven', 'elliot', 'ello', 'els', 'elsewher', 'elvi', 'em', 'email', 'embarass', 'embarrass', 'embassi', 'emerg', 'emigr', 'emili', 'emot', 'employ', 'employe', 'empti', 'en', 'enam', 'enc', 'end', 'endless', 'endof', 'endow', 'enemi', 'energi', 'eng', 'engag', 'engalnd', 'engin', 'england', 'english', 'enjoy', 'enjoyin', 'enketa', 'enna', 'ennal', 'enough', 'enter', 'entertain', 'entey', 'entir', 'entitl', 'entrepreneur', 'entri', 'entrop', 'enufcredeit', 'enuff', 'envelop', 'envi', 'epi', 'epsilon', 'equal', 'ericson', 'ericsson', 'erm', 'erot', 'err', 'error', 'ertini', 'eruku', 'erupt', 'erutupalam', 'eryth', 'esaplanad', 'escal', 'escap', 'ese', 'eshxxxxxxxxxxx', 'especi', 'espel', 'esplanad', 'essay', 'essenti', 'establish', 'eta', 'etc', 'etern', 'ethnic', 'ethreat', 'ettan', 'euro', 'euro2004', 'eurodisinc', 'europ', 'evalu', 'evapor', 'eve', 'eveb', 'evei', 'even', 'event', 'eventu', 'ever', 'everi', 'every1', 'everybodi', 'everyboy', 'everyday', 'everyon', 'everyso', 'everyth', 'everythin', 'everytim', 'everywher', 'evey', 'evict', 'evil', 'evn', 'evng', 'evo', 'evon', 'evr', 'evrey', 'evri', 'evry1', 'evrydi', 'ex', 'exact', 'exactli', 'exam', 'excel', 'except', 'exchang', 'excit', 'excus', 'exe', 'execut', 'exercis', 'exet', 'exhaust', 'exhibit', 'exist', 'exmpel', 'exorc', 'exorcist', 'exp', 'expect', 'expens', 'experi', 'experiencehttpwwwvouch4mecometlpdiningasp', 'expert', 'expir', 'expiredso', 'expiri', 'explain', 'explicit', 'explicitli', 'explos', 'expos', 'express', 'ext', 'extermin', 'extra', 'extract', 'extrem', 'exwif', 'ey', 'eye', 'eyeddont', 'f', 'fab', 'faber', 'face', 'faceasssssholeee', 'facebook', 'facil', 'fact', 'factori', 'fade', 'faggi', 'faglord', 'fail', 'failur', 'faint', 'fair', 'faith', 'faitheven', 'fake', 'fakemi', 'fakey', 'fal', 'falconerf', 'fall', 'fallen', 'famamu', 'famili', 'familiar', 'familymay', 'famou', 'fan', 'fanci', 'fantasi', 'fantast', 'far', 'farm', 'farrel', 'fart', 'fassyol', 'fast', 'faster', 'fastest', 'fastpl', 'fat', 'fate', 'father', 'fathima', 'fatti', 'fault', 'faultal', 'faultf', 'fav', 'fave', 'favor', 'favorit', 'favour', 'favourit', 'fb', 'fear', 'featheri', 'featur', 'feb', 'febapril', 'februari', 'fedex', 'fee', 'feed', 'feel', 'feelin', 'feelingood', 'feelingwav', 'feet', 'fell', 'fellow', 'felt', 'femal', 'feng', 'festiv', 'fetch', 'fever', 'few', 'fffff', 'ffffffffff', 'ffffuuuuuuu', 'fgkslpo', 'fgkslpopw', 'fidalf', 'field', 'fieldof', 'fiendmak', 'fifa', 'fifteen', 'fifth', 'fifti', 'fight', 'fightng', 'figur', 'file', 'fill', 'film', 'filth', 'filthi', 'filthyguy', 'final', 'finalis', 'financ', 'financi', 'find', 'fine', 'fineabsolutli', 'fineinshah', 'finest', 'finewhen', 'finger', 'finish', 'finishd', 'fink', 'finn', 'fire', 'firefox', 'fireplac', 'firesar', 'firmwar', 'firsg', 'first', 'fish', 'fishhead', 'fishrman', 'fit', 'fite', 'five', 'fix', 'fixd', 'fixedlin', 'fizz', 'flag', 'flake', 'flaki', 'flame', 'flash', 'flat', 'flatter', 'flavour', 'flea', 'fletcher', 'flew', 'fli', 'flight', 'flim', 'flip', 'flippin', 'flirt', 'float', 'flood', 'floor', 'floppi', 'florida', 'flow', 'flower', 'fluid', 'flung', 'flurri', 'flute', 'flyim', 'flyng', 'fml', 'fmyou', 'fne', 'fo', 'fold', 'foley', 'folk', 'follow', 'followin', 'fond', 'fondli', 'fone', 'fonin', 'food', 'fool', 'foot', 'footbal', 'footblcrckt', 'footi', 'footprint', 'for', 'forc', 'foreg', 'foreign', 'forev', 'forevr', 'forfeit', 'forget', 'forgiv', 'forgiven', 'forgot', 'forgotten', 'forgt', 'form', 'formal', 'formallypl', 'format', 'formclark', 'formsdon', 'forth', 'fortun', 'forum', 'forward', 'found', 'foundurself', 'four', 'fourth', 'foward', 'fowler', 'fox', 'fp', 'fr', 'fraction', 'fran', 'frankgood', 'franki', 'franxx', 'franyxxxxx', 'fraud', 'freak', 'freaki', 'fredericksburg', 'free', 'free2day', 'freedom', 'freeentri', 'freefon', 'freek', 'freeli', 'freemessag', 'freemsg', 'freemsgfav', 'freemsgfeelin', 'freenokia', 'freephon', 'freerington', 'freeringtonerepli', 'freesend', 'freez', 'freind', 'fren', 'french', 'frequent', 'fresh', 'fresher', 'fret', 'fri', 'friday', 'fridayhop', 'fridg', 'friend', 'friendofafriend', 'friendsar', 'friendship', 'friendshipmotherfatherteacherschildren', 'fring', 'frm', 'frmcloud', 'frnd', 'frndship', 'frndshp', 'frndsship', 'frndz', 'frnt', 'fro', 'frog', 'frogaxel', 'from', 'fromm', 'fromwrk', 'front', 'frontiervil', 'frosti', 'fruit', 'frwd', 'ft', 'fuck', 'fuckin', 'fuckinniceselfishdeviousbitchanywayi', 'fudg', 'fuell', 'fujitsu', 'ful', 'fulfil', 'full', 'fullonsmscom', 'fumbl', 'fun', 'function', 'fund', 'fundament', 'funer', 'funk', 'funki', 'funni', 'furnitur', 'further', 'fusion', 'futur', 'fuuuuck', 'fwiw', 'fyi', 'g', 'g696ga', 'ga', 'gail', 'gailxx', 'gain', 'gal', 'galcan', 'galileo', 'galno', 'galsu', 'gam', 'game', 'gamestar', 'gandhipuram', 'ganesh', 'gang', 'gap', 'garag', 'garbag', 'garden', 'gari', 'garment', 'gastroenter', 'gate', 'gaug', 'gautham', 'gave', 'gay', 'gayd', 'gayl', 'gaytextbuddycom', 'gaze', 'gbp', 'gbp150week', 'gbp450week', 'gbp5month', 'gbpsm', 'gbpweek', 'gd', 'gdeve', 'gdnow', 'gdthe', 'ge', 'gee', 'geeee', 'geeeee', 'geelat', 'gei', 'gek1510', 'gender', 'gene', 'gener', 'geniu', 'gent', 'gentl', 'gentleman', 'gentli', 'genu', 'genuin', 'geoenvironment', 'georg', 'gep', 'ger', 'germani', 'get', 'get4an18th', 'gete', 'geti', 'getsleep', 'getstop', 'gettin', 'getzedcouk', 'gf', 'ghodbandar', 'ghost', 'gibb', 'gibe', 'gift', 'giggl', 'gigolo', 'gimm', 'gimmi', 'gin', 'girl', 'girld', 'girlfrnd', 'girli', 'gist', 'giv', 'give', 'given', 'givit', 'glad', 'gland', 'glasgow', 'glass', 'glo', 'global', 'glori', 'gloriou', 'gloucesterroad', 'gmgngegn', 'gmgngegnt', 'gmw', 'gnarl', 'go', 'go2', 'go2sri', 'goa', 'goal', 'goalsteam', 'gobi', 'god', 'godi', 'godnot', 'godtaken', 'godyou', 'goe', 'goggl', 'goigng', 'goin', 'goin2b', 'gokila', 'gold', 'golddigg', 'golden', 'goldvik', 'golf', 'gon', 'gona', 'gone', 'goneu', 'gong', 'gonna', 'gonnamissu', 'good', 'gooddhanush', 'goodenviron', 'goodeven', 'goodfin', 'goodfriend', 'goodi', 'goodmat', 'goodmorn', 'goodmorningmi', 'goodnight', 'goodnit', 'goodno', 'goodnoon', 'goodo', 'goodtimeoli', 'goodwhen', 'googl', 'gopalettan', 'gorgeou', 'gosh', 'gossip', 'gossx', 'got', 'gota', 'gotani', 'gotmarri', 'goto', 'gotta', 'gotten', 'gotto', 'gover', 'govtinstituit', 'gowait', 'gower', 'gpr', 'gpu', 'gr8', 'gr8fun', 'gr8prize', 'grab', 'grace', 'graduat', 'grahmbel', 'gram', 'gran', 'grand', 'grandfath', 'grandma', 'granit', 'grant', 'graphic', 'grasp', 'grate', 'grave', 'gravel', 'gravi', 'graviti', 'gray', 'graze', 'gre', 'great', 'greatbhaji', 'greatby', 'greatest', 'greatli', 'greec', 'green', 'greeni', 'greet', 'grief', 'grin', 'grinder', 'grinul', 'grl', 'grocer', 'groov', 'groovi', 'ground', 'groundamla', 'group', 'grow', 'grown', 'grownup', 'growrandom', 'grr', 'grumbl', 'grumpi', 'gs', 'gsex', 'gsoh', 'gt', 'gua', 'guai', 'guarante', 'gucci', 'gud', 'gudk', 'gudni8', 'gudnit', 'gudnitetcpractic', 'gudnyt', 'guess', 'guessin', 'guid', 'guidanc', 'guild', 'guilti', 'guitar', 'gumbi', 'guoyang', 'gurl', 'gut', 'guy', 'gv', 'gving', 'gwr', 'gym', 'gymnast', 'gyna', 'gyno', 'h', 'ha', 'habbahw', 'habit', 'hack', 'had', 'hadnt', 'hadya', 'haf', 'haha', 'hahahaus', 'hahatak', 'hai', 'hail', 'hair', 'haircut', 'hairdress', 'haiyoh', 'haiz', 'half', 'half8th', 'hall', 'halla', 'hallaq', 'halloween', 'ham', 'hamper', 'hamster', 'hand', 'handl', 'handset', 'handsom', 'hang', 'hanger', 'hangin', 'hank', 'hannaford', 'hanumanji', 'happen', 'happend', 'happenin', 'happi', 'happier', 'happiest', 'happili', 'hard', 'hardcor', 'harder', 'hardest', 'hardli', 'hari', 'harish', 'harlem', 'harri', 'hasbroin', 'hasnt', 'hassl', 'hat', 'hate', 'haughaighgtujhyguj', 'haul', 'haunt', 'hav', 'hav2hear', 'hava', 'havbeen', 'have', 'havebeen', 'haven', 'havent', 'haventcn', 'havin', 'havnt', 'hcl', 'hdd', 'he', 'head', 'headach', 'headin', 'headset', 'headstart', 'heal', 'healer', 'healthi', 'heap', 'hear', 'heard', 'hearin', 'heart', 'heartgn', 'heartheart', 'heartsnot', 'heat', 'heater', 'heaven', 'heavi', 'heavili', 'hectic', 'hee', 'heehe', 'hehe', 'height', 'held', 'helen', 'hell', 'hella', 'hello', 'hellodrivby0quit', 'hellogorg', 'hellohow', 'helloooo', 'helloy', 'help', 'help08700469649', 'help08700621170150p', 'help08712400602450p', 'help08714742804', 'help08718728876', 'helplin', 'heltiniiyo', 'hen', 'henc', 'henri', 'hep', 'her', 'here', 'herepl', 'hererememb', 'herethanksi', 'heri', 'herlov', 'hermi', 'hero', 'heroi', 'heron', 'herself', 'hersh', 'herwho', 'herwil', 'hesit', 'hex', 'hey', 'heygreat', 'hgsuite3422land', 'hgsuite3422landsroww1j6hl', 'hhahhaahahah', 'hi', 'hict', 'hidden', 'hide', 'hidid', 'high', 'highest', 'hii', 'hilariousalso', 'hill', 'hillsborough', 'him', 'himself', 'himso', 'himthen', 'hint', 'hip', 'hiphop', 'hire', 'hisher', 'histori', 'hit', 'hitechn', 'hitler', 'hitman', 'hitteranyway', 'hittng', 'hiwhat', 'hiya', 'hlday', 'hlp', 'hme', 'hmm', 'hmmbad', 'hmmm', 'hmmmbut', 'hmmmhow', 'hmmmi', 'hmmmkbut', 'hmmmm', 'hmmmstill', 'hmph', 'hmv', 'hmv1', 'ho', 'hockey', 'hogidhechinnu', 'hogli', 'hogolo', 'hol', 'holbi', 'hold', 'holder', 'hole', 'holi', 'holiday', 'holidayso', 'holla', 'hollalat', 'home', 'homebut', 'homecheck', 'homeleft', 'homelov', 'homeown', 'homewot', 'hon', 'honest', 'honesti', 'honestli', 'honey', 'honeybe', 'honeydid', 'honeymoon', 'honi', 'hont', 'hoo', 'hooch', 'hoodi', 'hook', 'hoop', 'hop', 'hope', 'hopeafternoon', 'hopeso', 'hopeu', 'hor', 'horni', 'horniest', 'horo', 'horribl', 'hors', 'hospit', 'hostbas', 'hostel', 'hostil', 'hot', 'hotel', 'hotmix', 'hottest', 'hour', 'hourish', 'hous', 'housemaid', 'housew', 'housework', 'how', 'howard', 'howda', 'howdi', 'howev', 'howr', 'howu', 'howv', 'howz', 'hp', 'hp20', 'hppnss', 'hr', 'hrishi', 'hsbc', 'html', 'httpalto18coukwavewaveaspo44345', 'httpcareer', 'httpdoit', 'httpgotbabescouk', 'httpimg', 'httptm', 'httpwap', 'httpwwwbubbletextcom', 'httpwwwetlpcoukexpressoff', 'httpwwwetlpcoukreward', 'httpwwwgr8prizescom', 'httpwwwurawinnercom', 'httpwwwwtlpcouktext', 'huai', 'hubbi', 'hudgi', 'hug', 'huge', 'hugh', 'huh', 'hui', 'huim', 'hum', 'human', 'hun', 'hundr', 'hundredh', 'hungov', 'hungri', 'hunk', 'hunlov', 'hunni', 'hunnyhop', 'hunnyjust', 'hunnywot', 'hunonbu', 'hunt', 'hurri', 'hurrican', 'hurt', 'husband', 'hussey', 'hustl', 'hut', 'hv', 'hvae', 'hw', 'hwd', 'hwkeep', 'hyde', 'hypertens', 'hypotheticalhuagauahahuagahyuhagga', 'i', 'iZ', 'ia', 'iam', 'ibh', 'ibhltd', 'ibiza', 'ibm', 'ibn', 'ibor', 'ibuprofen', 'ic', 'iccha', 'ice', 'icic', 'icicibankcom', 'icki', 'icon', 'id', 'idc', 'idconvey', 'idea', 'ideal', 'identif', 'identifi', 'idiot', 'idk', 'idp', 'idu', 'ie', 'if', 'iff', 'ifink', 'ifwhenhow', 'ig11', 'ignor', 'ijust', 'ikea', 'ikno', 'iknow', 'il', 'ileav', 'ill', 'illspeak', 'ilol', 'im', 'ima', 'imag', 'imagin', 'imaginationmi', 'imat', 'imf', 'imin', 'imma', 'immedi', 'immunis', 'imp', 'impati', 'implic', 'import', 'importantli', 'impos', 'imposs', 'impost', 'impress', 'improv', 'imprtant', 'in', 'in2', 'inc', 'inch', 'incid', 'inclu', 'includ', 'inclus', 'incomm', 'inconsider', 'inconveni', 'incorrect', 'increas', 'incred', 'increment', 'ind', 'inde', 'independ', 'india', 'indian', 'indianpl', 'indic', 'individu', 'individualtim', 'indyarockscom', 'inev', 'infact', 'infect', 'infern', 'influx', 'info', 'inforingtonekingcouk', 'inform', 'informedrgdsrakheshkerala', 'infotxt82228couk', 'infovipclub4u', 'infowww100percentrealcom', 'infra', 'infront', 'ing', 'ingredi', 'initi', 'ink', 'inlud', 'inmind', 'inner', 'inning', 'innoc', 'innu', 'inour', 'inperialmus', 'inperson', 'inr', 'insect', 'insha', 'inshah', 'insid', 'inspect', 'inst', 'instal', 'instant', 'instantli', 'instead', 'instruct', 'insur', 'intellig', 'intend', 'intent', 'interest', 'interflora', 'interfu', 'intern', 'internet', 'internetservic', 'interview', 'interviw', 'intha', 'intim', 'into', 'intrepid', 'intro', 'intrud', 'invad', 'invent', 'invest', 'investig', 'invit', 'invnt', 'invoic', 'involv', 'iouri', 'ip', 'ip4', 'ipad', 'ipaditan', 'iphon', 'ipod', 'iraq', 'ireneer', 'iriv', 'iron', 'irrit', 'irulina', 'is', 'isaiahd', 'isar', 'iscom', 'ish', 'ishtamayoohappi', 'island', 'islov', 'isn', 'isnt', 'issu', 'isvimport', 'it', 'italian', 'itboth', 'itc', 'itcould', 'item', 'iter', 'ithi', 'ithink', 'iti', 'itjust', 'itleav', 'itlet', 'itll', 'itmail', 'itmay', 'itna', 'itnow', 'itor', 'itplspl', 'itried2tel', 'itself', 'itsnot', 'ittb', 'itu', 'itwhichturnedinto', 'itxt', 'itxx', 'itz', 'ivatt', 'ive', 'iwana', 'iwasmarinethat', 'iz', 'izzit', 'j', 'j89', 'jabo', 'jack', 'jacket', 'jackpot', 'jackson', 'jacuzzi', 'jada', 'jade', 'jaklin', 'jam', 'jame', 'jamster', 'jamstercouk', 'jamsterget', 'jamz', 'jan', 'janarig', 'jane', 'janinexx', 'januari', 'janx', 'jap', 'japanes', 'jason', 'java', 'jay', 'jaya', 'jaykwon', 'jaz', 'jazz', 'jb', 'je', 'jealou', 'jean', 'jeetey', 'jeevithathil', 'jelli', 'jen', 'jenn', 'jenni', 'jenxxx', 'jeremiah', 'jeri', 'jerk', 'jerri', 'jersey', 'jess', 'jesu', 'jet', 'jetton', 'jewelri', 'jez', 'ji', 'jia', 'jiayin', 'jide', 'jiu', 'jjc', 'jo', 'joanna', 'job', 'jobyet', 'jock', 'jod', 'jog', 'john', 'join', 'joinedhop', 'joinedso', 'joke', 'joker', 'jokethet', 'jokin', 'jolli', 'jolt', 'jon', 'jone', 'jontin', 'jordan', 'jordantxt', 'jorgeshock', 'jot', 'journey', 'joy', 'jp', 'js', 'jsco', 'jst', 'jstfrnd', 'jsut', 'ju', 'juan', 'judgementali', 'juici', 'jule', 'juli', 'juliana', 'julianaland', 'jump', 'jumper', 'june', 'jungl', 'junna', 'just', 'justbeen', 'justifi', 'justthought', 'juswok', 'juz', 'k', 'k52', 'k61', 'k718', 'kaaj', 'kadeem', 'kafter', 'kaiez', 'kaila', 'kaitlyn', 'kalaachutaarama', 'kalainar', 'kalisidar', 'kall', 'kalli', 'kalstiyathen', 'kama', 'kanagu', 'kane', 'kanji', 'kano', 'kanoanyway', 'kanoil', 'kanowhr', 'kappa', 'karaok', 'karnan', 'karo', 'kate', 'katexxx', 'kath', 'kavalan', 'kay', 'kaypoh', 'kb', 'kbut', 'kdo', 'ke', 'keen', 'keep', 'keepintouch', 'kegger', 'keluviri', 'ken', 'keng', 'kent', 'kept', 'kerala', 'keralacircl', 'keri', 'kettoda', 'key', 'keypad', 'keyword', 'kfc', 'kg', 'kgive', 'kgood', 'khelat', 'ki', 'kicchu', 'kick', 'kickbox', 'kickoff', 'kid', 'kidz', 'kill', 'kilo', 'kim', 'kind', 'kinda', 'kindli', 'king', 'kingdom', 'kintu', 'kiosk', 'kip', 'kisi', 'kiss', 'kit', 'kitti', 'kittum', 'kkadvanc', 'kkani', 'kkapo', 'kkare', 'kkcongratul', 'kkfrom', 'kkgoodstudi', 'kkhow', 'kkim', 'kkit', 'kkthi', 'kkwhat', 'kkwhen', 'kkwhere', 'kkwhi', 'kkyesterday', 'kl341', 'knacker', 'knee', 'knew', 'knicker', 'knock', 'know', 'knowh', 'known', 'knowneway', 'knowthi', 'knowwait', 'knowyetund', 'knw', 'ko', 'kochi', 'kodstini', 'kodthini', 'konw', 'korch', 'korean', 'korli', 'kort', 'kote', 'kothi', 'ksri', 'kthen', 'ktv', 'kuch', 'kudiyarasu', 'kusruthi', 'kvb', 'kwish', 'kyou', 'kz', 'l8', 'l8er', 'l8r', 'l8tr', 'la', 'la1', 'la3', 'la32wu', 'lab', 'labor', 'lac', 'lack', 'lacsthat', 'lacsther', 'laden', 'ladi', 'ladiesu', 'lag', 'lage', 'lager', 'laid', 'laidwant', 'lakh', 'lambda', 'lambu', 'lamp', 'lancast', 'land', 'landlin', 'landlineonli', 'landmark', 'lane', 'langport', 'languag', 'lanka', 'lanr', 'lap', 'lapdanc', 'laptop', 'lar', 'lara', 'lareadi', 'larg', 'largest', 'lark', 'lasagna', 'last', 'lastest', 'late', 'latebut', 'latei', 'latelyxxx', 'later', 'lateso', 'latest', 'latr', 'laugh', 'laundri', 'lauri', 'lautech', 'lavend', 'law', 'laxinorf', 'lay', 'layin', 'lazi', 'lccltd', 'ldn', 'ldnw15h', 'le', 'lead', 'leadership', 'leafcutt', 'leafdayno', 'leagu', 'leannewhat', 'learn', 'least', 'least5tim', 'leastwhich', 'leav', 'lect', 'lectur', 'left', 'leftov', 'leg', 'legal', 'legitimat', 'leh', 'lehhaha', 'lei', 'lekdog', 'lemm', 'length', 'lennon', 'leo', 'leona', 'leonardo', 'less', 'lesser', 'lesson', 'let', 'letter', 'leu', 'level', 'li', 'liao', 'liaoso', 'liaotoo', 'lib', 'libertin', 'librari', 'lick', 'lido', 'lie', 'life', 'lifeand', 'lifebook', 'lifei', 'lifethi', 'lifetim', 'lifey', 'lifpartnr', 'lift', 'light', 'lighter', 'lightli', 'lik', 'like', 'likeyour', 'likingb', 'lil', 'lili', 'lim', 'limit', 'limp', 'lindsay', 'line', 'linear', 'linerent', 'liney', 'lingeri', 'lingo', 'link', 'linux', 'lion', 'lionm', 'lionp', 'lip', 'lipo', 'liquor', 'list', 'listen', 'listening2th', 'listn', 'lit', 'liter', 'litr', 'littl', 'live', 'liver', 'liverpool', 'lk', 'lkpobox177hp51fl', 'll', 'llspeak', 'lm', 'lmao', 'lmaonic', 'lnli', 'lo', 'load', 'loan', 'lobbi', 'local', 'locat', 'locaxx', 'lock', 'lodg', 'log', 'login', 'logo', 'logoff', 'logon', 'logop', 'logosmusicnew', 'loko', 'lol', 'lolnic', 'lololo', 'londn', 'london', 'lone', 'loneli', 'long', 'longer', 'lonlin', 'loo', 'look', 'lookatm', 'lookin', 'lool', 'loooooool', 'looovvv', 'loos', 'loosu', 'lor', 'lord', 'lorgoin', 'lorw', 'lose', 'loser', 'loss', 'lost', 'lot', 'loti', 'lotr', 'lotsli', 'lotsof', 'lotta', 'lotto', 'lotwil', 'lotz', 'lou', 'loud', 'loung', 'lousi', 'lov', 'lovabl', 'love', 'loveabl', 'lovejen', 'lovem', 'lover', 'loverakhesh', 'loverboy', 'lovin', 'lovingli', 'lovli', 'low', 'lowcost', 'lower', 'loxahatche', 'loyal', 'loyalti', 'lrg', 'ls1', 'ls15hb', 'ls278bb', 'lst', 'lt', 'lt3', 'ltd', 'ltdecimalgt', 'ltdhelpdesk', 'ltemailgt', 'ltgt', 'lttimegt', 'lttr', 'lturlgt', 'lubli', 'luci', 'luck', 'luck2', 'lucki', 'luckili', 'lucozad', 'lucozadecoukwrc', 'lucyxx', 'luk', 'lul', 'lunch', 'lunchtim', 'lunchyou', 'lunsford', 'lush', 'luton', 'luv', 'luvd', 'luvnight', 'lux', 'luxuri', 'lv', 'lvblefrnd', 'lyf', 'lyfu', 'lyk', 'lyric', 'lyricalladie21f', 'm', 'm100', 'm221bp', 'm227xi', 'm26', 'm263uz', 'm39m51', 'm8', 'm95', 'ma', 'maaaan', 'maangalyam', 'maat', 'mac', 'macedonia', 'macha', 'machan', 'machiani', 'machin', 'macho', 'mack', 'macleran', 'mad', 'mad1', 'mad2', 'madam', 'madamregret', 'made', 'madodu', 'madok', 'madstini', 'madthen', 'mag', 'maga', 'magazin', 'maggi', 'magic', 'magicalsongsblogspotcom', 'mah', 'mahal', 'mahfuuzmean', 'mail', 'mailbox', 'maili', 'main', 'maintain', 'major', 'make', 'maki', 'makin', 'malaria', 'malarki', 'male', 'mall', 'mallika', 'man', 'manag', 'manchest', 'manda', 'mandan', 'mandara', 'mandi', 'maneesha', 'maneg', 'mango', 'mani', 'maniac', 'manki', 'manual', 'map', 'mapquest', 'maraikara', 'marandratha', 'march', 'maretar', 'margaret', 'margin', 'mari', 'mark', 'market', 'marley', 'marrgeremembr', 'marri', 'marriag', 'marriageprogram', 'marsm', 'marvel', 'mask', 'massag', 'massagetiepo', 'massiv', 'master', 'masteriast', 'mat', 'match', 'mate', 'math', 'mathemat', 'mathew', 'matra', 'matric', 'matrix3', 'matter', 'mattermsg', 'matthew', 'matur', 'max', 'max10min', 'max6month', 'maxim', 'maximum', 'may', 'mayb', 'mb', 'mc', 'mca', 'mcat', 'mcr', 'me', 'meal', 'mean', 'meaning', 'meaningless', 'meant', 'meanwhil', 'mear', 'measur', 'meat', 'meatbal', 'mecaus', 'med', 'medic', 'medicin', 'medont', 'mee', 'meet', 'meetgreet', 'meetin', 'meetitz', 'mega', 'meh', 'mei', 'meim', 'meiv', 'mel', 'melik', 'mell', 'melnit', 'melodi', 'melt', 'member', 'membership', 'membershiptak', 'memor', 'memori', 'men', 'mene', 'mental', 'mention', 'mentionedtomorrow', 'mentor', 'menu', 'meok', 'meow', 'meowd', 'mere', 'merememberin', 'meremov', 'merri', 'mesag', 'mesh', 'meso', 'mess', 'messag', 'messageit', 'messageno', 'messagepandi', 'messagesim', 'messagesom', 'messagestext', 'messagethank', 'messeng', 'messi', 'met', 'method', 'meummifyingby', 'mfl', 'mg', 'mi', 'mia', 'michael', 'mid', 'middl', 'midnight', 'might', 'miiiiiiissssssssss', 'mila', 'mile', 'mileag', 'milk', 'milkdayno', 'miller', 'million', 'miltazindgi', 'min', 'mina', 'minapn', 'mind', 'mindi', 'mindsetbeliev', 'mine', 'mineal', 'minecraft', 'mini', 'minimum', 'minnaminungint', 'minor', 'mins100txtmth', 'minstand', 'minstext', 'mint', 'minu', 'minut', 'miracl', 'mirror', 'misbehav', 'mise', 'miser', 'misfit', 'misplac', 'miss', 'misscal', 'missi', 'missin', 'mission', 'missionari', 'misss', 'misstak', 'missunderstd', 'mist', 'mistak', 'mistakeu', 'misundrstud', 'mite', 'mitsak', 'mittelschmertz', 'miwa', 'mix', 'mj', 'mjzgroup', 'mk17', 'mk45', 'ml', 'mmm', 'mmmm', 'mmmmm', 'mmmmmm', 'mmmmmmm', 'mmsto', 'mn', 'mnth', 'mo', 'moan', 'mob', 'mobcudb', 'mobi', 'mobil', 'mobilesdirect', 'mobilesvari', 'mobileupd8', 'mobno', 'mobsicom', 'mobstorequiz10ppm', 'mode', 'model', 'modelsoni', 'modl', 'modul', 'mofo', 'moji', 'mojibiola', 'mokka', 'molestedsomeon', 'mom', 'moment', 'mon', 'monday', 'mondaynxt', 'moneeppolum', 'money', 'moneya', 'moneyi', 'monkeespeopl', 'monkey', 'monkeyaround', 'monl8rsx', 'mono', 'monoc', 'monster', 'month', 'monthli', 'monthlysubscription50pmsg', 'monthnot', 'mood', 'moon', 'moral', 'moraldont', 'moralon', 'more', 'morn', 'mornin', 'morningtak', 'morphin', 'morrow', 'moseley', 'most', 'mostli', 'mother', 'motherfuck', 'motherinlaw', 'motiv', 'motor', 'motorola', 'mountain', 'mous', 'mouth', 'move', 'movi', 'moviewat', 'moyep', 'mp3', 'mquiz', 'mr', 'mre', 'mrng', 'mrt', 'mrur', 'ms', 'msg', 'msg150p', 'msging', 'msgrcvd18', 'msgs150p', 'msgsd', 'msgsometext', 'msgsubscript', 'msgticketkioskvalid', 'msgwe', 'msn', 'mssuman', 'mt', 'mtalk', 'mth', 'mtnl', 'mu', 'much', 'muchand', 'muchi', 'muchimped', 'muchxxlov', 'mudyadhu', 'mufti', 'muhommad', 'muht', 'multi', 'multimedia', 'multipli', 'mum', 'mumbai', 'mumha', 'mummi', 'mumtaz', 'mundh', 'munster', 'murali', 'murder', 'mush', 'mushi', 'music', 'must', 'musta', 'musthu', 'mustprovid', 'mutai', 'mutat', 'muz', 'mw', 'mwah', 'my', 'mycallsu', 'mylif', 'mymobi', 'mypar', 'myself', 'myspac', 'mysteri', 'mytonecomenjoy', 'n', 'n8', 'na', 'naal', 'nacho', 'nag', 'nagar', 'nah', 'nahi', 'nail', 'nake', 'nalla', 'nalli', 'name', 'name1', 'name2', 'namemi', 'nammanna', 'nan', 'nang', 'nanni', 'nap', 'narcot', 'nasdaq', 'naseeb', 'nasti', 'nat', 'natali', 'natalja', 'nation', 'nationwid', 'nattil', 'natuit', 'natur', 'natwest', 'naughti', 'nauseou', 'nav', 'navig', 'nb', 'nbme', 'nd', 'ne', 'near', 'nearbi', 'nearer', 'nearli', 'neces', 'necess', 'necessari', 'necessarili', 'neck', 'necklac', 'ned', 'need', 'needa', 'neededsalari', 'needi', 'needl', 'neekunna', 'neft', 'neg', 'neglect', 'neglet', 'neighbor', 'neither', 'nelson', 'neo69', 'nervou', 'neshanthtel', 'net', 'netcollex', 'netflix', 'neth', 'netno', 'network', 'neva', 'nevamindw', 'never', 'nevil', 'nevr', 'new', 'neway', 'newest', 'newport', 'newquaysend', 'news', 'newsbi', 'newscast', 'newshyp', 'newspap', 'next', 'ngage', 'nh', 'ni8', 'ni8swt', 'nic', 'nice', 'nicenicehow', 'nichol', 'nick', 'nickey', 'nicki', 'nig', 'nigeria', 'nigh', 'night', 'nighter', 'nightnight', 'nightnobodi', 'nightsexcel', 'nightsw', 'nightswt', 'nigpun', 'nigro', 'nike', 'nikiyu4net', 'nimbomson', 'nimya', 'nimyapl', 'ninish', 'nino', 'nipost', 'nit', 'nite', 'nite2', 'nitro', 'nitw', 'nitz', 'njan', 'nmde', 'no', 'no1', 'no165', 'no434', 'no440', 'no762', 'no81151', 'no83355', 'no910', 'nob', 'nobl', 'nobodi', 'nobut', 'noe', 'nofew', 'nohe', 'noi', 'noic', 'nois', 'noisi', 'noit', 'nojst', 'nok', 'nokia', 'nokia150p', 'nokia6600', 'nokia6650', 'nolin', 'nolistened2th', 'non', 'noncomitt', 'none', 'nonenowher', 'nonetheless', 'nookii', 'noon', 'nooooooo', 'noooooooo', 'nope', 'nor', 'nora', 'norcorp', 'nordstrom', 'norm', 'norm150pton', 'normal', 'north', 'northampton', 'nose', 'nosh', 'nosi', 'not', 'note', 'notebook', 'noth', 'nothi', 'nothin', 'notic', 'notif', 'notifi', 'notixiqu', 'nottel', 'nottingham', 'notxtcouk', 'noun', 'novelti', 'novemb', 'now', 'now1', 'now4t', 'nowaday', 'nowadayslot', 'nowcan', 'nowi', 'nownyt', 'nowonion', 'noworriesloanscom', 'nowrepli', 'nowsavamobmemb', 'nowsend', 'nowski', 'nowstil', 'nowtc', 'nowus', 'nr31', 'nri', 'nt', 'nte', 'ntswt', 'ntt', 'ntwk', 'nu', 'nuclear', 'nudist', 'nuerologist', 'num', 'number', 'numberpl', 'numberrespect', 'numberso', 'nurs', 'nurseri', 'nurungu', 'nusstu', 'nuther', 'nutter', 'nver', 'nvm', 'nvq', 'nw', 'nxt', 'ny', 'nyc', 'nydc', 'nyt', 'nytec2a3lpmsg150p', 'nytho', 'nyusa', 'nz', 'nte', 'o', 'o2coukgam', 'o2fwd', 'oath', 'obedi', 'obes', 'obey', 'object', 'oblising', 'oblivi', 'obvious', 'occas', 'occupi', 'occur', 'oceand', 'oclock', 'octob', 'odalebeku', 'odi', 'of', 'ofcours', 'off', 'offc', 'offcampu', 'offdam', 'offens', 'offer', 'offerth', 'offic', 'officestil', 'officethenampet', 'officeunderstand', 'officewhat', 'offici', 'offlin', 'ofic', 'oficegot', 'ofsi', 'often', 'oga', 'ogunrind', 'oh', 'oha', 'ohi', 'oic', 'oil', 'oja', 'ok', 'okay', 'okcom', 'okday', 'okden', 'okey', 'oki', 'okmail', 'okok', 'okor', 'oktak', 'okthenwhat', 'okvarunnathu', 'ola', 'olag', 'olav', 'olayiwola', 'old', 'ollubut', 'olol', 'olowoyey', 'olymp', 'omg', 'omw', 'on', 'onam', 'onc', 'oncal', 'ondu', 'one', 'onedg', 'oneta', 'oni', 'onionr', 'onit', 'onli', 'onlin', 'onlinewhi', 'onluy', 'only1mor', 'onlybettr', 'onlydon', 'onlyfound', 'onto', 'onum', 'onward', 'onword', 'ooh', 'oooh', 'oooooh', 'ooooooh', 'oop', 'open', 'openin', 'oper', 'opinion', 'opp', 'opponent', 'opportun', 'opportunityal', 'opportunitypl', 'oppos', 'opposit', 'opt', 'optimist', 'optin', 'option', 'optout', 'or', 'or2optouthv9d', 'or2stoptxt', 'oral', 'orang', 'orangei', 'orc', 'orchard', 'order', 'ore', 'oredi', 'oreo', 'organ', 'organis', 'orh', 'orig', 'origin', 'orno', 'ortxt', 'oru', 'os', 'oscar', 'oso', 'otbox', 'other', 'otherwis', 'othr', 'otsid', 'ou', 'ouch', 'our', 'ourback', 'oursso', 'out', 'outag', 'outbid', 'outdoor', 'outfit', 'outfor', 'outgo', 'outhav', 'outif', 'outl8rjust', 'outrag', 'outreach', 'outsid', 'outsomewher', 'outstand', 'outta', 'ovarian', 'over', 'overa', 'overdid', 'overdos', 'overemphasiseor', 'overh', 'overtim', 'ovr', 'ovul', 'ovulatewhen', 'ow', 'owe', 'owl', 'own', 'ownyouv', 'owo', 'oxygen', 'oyea', 'oyster', 'oz', 'p', 'pa', 'pace', 'pack', 'packag', 'packalso', 'padhegm', 'page', 'pai', 'paid', 'pain', 'painhop', 'painit', 'paint', 'pale', 'palm', 'pan', 'panalambut', 'panason', 'pandi', 'panic', 'panick', 'panren', 'pansi', 'pant', 'panther', 'panti', 'pap', 'papa', 'paper', 'paperwork', 'paracetamol', 'parachut', 'parad', 'paragon', 'paragraph', 'paranoid', 'parantella', 'parchi', 'parco', 'parent', 'parentnot', 'parentsi', 'pari', 'parisfre', 'parish', 'park', 'park6ph', 'parkin', 'part', 'parti', 'particip', 'particular', 'particularli', 'partner', 'partnership', 'paru', 'pase', 'pass', 'passabl', 'passion', 'passport', 'passthey', 'password', 'passwordsatmsm', 'past', 'pataistha', 'patent', 'path', 'pathaya', 'patient', 'patrick', 'pattern', 'patti', 'paul', 'paus', 'pay', 'payasam', 'payback', 'paye', 'payed2day', 'payment', 'payoh', 'paypal', 'pc', 'pdatenow', 'peac', 'peach', 'peak', 'pear', 'pee', 'peep', 'pehl', 'pei', 'pen', 'penc', 'pend', 'pendent', 'pendingi', 'peni', 'penni', 'peopl', 'per', 'percent', 'percentag', 'perf', 'perfect', 'perform', 'perfum', 'perhap', 'peril', 'period', 'peripher', 'perman', 'permiss', 'perpetu', 'persev', 'persian', 'person', 'person2di', 'personmeet', 'perspect', 'perumbavoor', 'peski', 'pest', 'pete', 'petei', 'petexxx', 'petey', 'peteynoi', 'petrol', 'petrolr', 'pg', 'ph', 'ph08700435505150p', 'ph08704050406', 'pharmaci', 'phase', 'phd', 'phew', 'phil', 'philosoph', 'philosophi', 'phne', 'phoenix', 'phone', 'phone750', 'phonebook', 'phoni', 'photo', 'photoshop', 'php', 'phrase', 'physic', 'piah', 'pic', 'pick', 'pickl', 'picsfree1', 'pictur', 'pictxt', 'pie', 'piec', 'pierr', 'pig', 'piggi', 'pilat', 'pile', 'pillow', 'pimpl', 'pimpleseven', 'pin', 'pink', 'pinku', 'pint', 'pisc', 'piss', 'piti', 'pix', 'pixel', 'pizza', 'pl', 'place', 'placement', 'placeno', 'plaid', 'plan', 'plane', 'planet', 'planeti', 'planettalkinstantcom', 'plate', 'platt', 'play', 'player', 'playerwhi', 'playi', 'playin', 'playng', 'plaza', 'pleas', 'pleasant', 'pleassssssseeeee', 'pleasur', 'plenti', 'plm', 'plough', 'plsi', 'plu', 'plum', 'plumber', 'plumbingremix', 'plural', 'plyr', 'plz', 'pm', 'pmt', 'po', 'po19', 'pobox', 'pobox1', 'pobox11414tcrw1', 'pobox12n146tf15', 'pobox12n146tf150p', 'pobox202', 'pobox334', 'pobox36504w45wq', 'pobox365o4w45wq', 'pobox45w2tg150p', 'pobox75ldns7', 'pobox84', 'poboxox36504w45wq', 'pocay', 'poci', 'pock', 'pocket', 'pocketbabecouk', 'pod', 'poem', 'poet', 'point', 'poke', 'poker', 'pokkiri', 'pole', 'poli', 'polic', 'politician', 'polo', 'poly200p', 'poly3', 'polyc', 'polyh', 'polyph', 'polyphon', 'polytruepixringtonesgam', 'pongal', 'pongaldo', 'ponnungal', 'poo', 'pooki', 'pool', 'poop', 'poor', 'poorli', 'poortiyagi', 'pop', 'popcorn', 'popcornjust', 'porn', 'porridg', 'port', 'portal', 'porteg', 'portion', 'pose', 'posh', 'posibl', 'posit', 'possess', 'possibl', 'possiblehop', 'post', 'postal', 'postcard', 'postcod', 'posterod', 'postpon', 'potato', 'potenti', 'potter', 'pouch', 'pound', 'pour', 'pout', 'power', 'poyyarikaturkolathupalayamunjalur', 'ppl', 'pple', 'pple700', 'ppm', 'ppm150', 'ppt150x3normal', 'prabha', 'prabhaim', 'prabu', 'pract', 'practic', 'practicum', 'practis', 'prais', 'prakasam', 'prakasamanu', 'prakesh', 'prap', 'prasad', 'prasanth', 'prashanthettan', 'pray', 'prayer', 'prayingwil', 'prayr', 'pre', 'prebook', 'predict', 'prefer', 'prem', 'premaricakindli', 'premier', 'premium', 'prepaid', 'prepar', 'prepay', 'prepon', 'preschoolcoordin', 'prescrib', 'prescripiton', 'prescript', 'presenc', 'present', 'presid', 'presley', 'presnt', 'press', 'pressi', 'pressur', 'prestig', 'pretend', 'pretsorginta', 'pretsovru', 'pretti', 'prevent', 'preview', 'previou', 'previous', 'prey', 'price', 'priceso', 'pride', 'priest', 'prin', 'princ', 'princegn', 'princess', 'print', 'printer', 'prior', 'prioriti', 'priscilla', 'privaci', 'privat', 'prix', 'priya', 'prize', 'prizeawait', 'prizeswith', 'prizeto', 'pro', 'prob', 'probabl', 'problem', 'problemat', 'problembut', 'problemfre', 'problemi', 'problm', 'problum', 'probthat', 'process', 'processexcel', 'processit', 'processnetwork', 'prod', 'product', 'prof', 'profession', 'professor', 'profil', 'profit', 'program', 'progress', 'project', 'prolli', 'prometazin', 'promin', 'promis', 'promo', 'promot', 'prompt', 'promptli', 'prone', 'proof', 'proov', 'prop', 'proper', 'properli', 'properti', 'propos', 'propsd', 'prospect', 'protect', 'prove', 'proverb', 'provid', 'provinc', 'proze', 'prsn', 'ps3', 'pshewmiss', 'psp', 'psxtra', 'psychiatrist', 'psychic', 'psychologist', 'pt2', 'ptbo', 'pthi', 'pub', 'pubcaf', 'public', 'publish', 'pudunga', 'pull', 'pump', 'punch', 'punish', 'punto', 'puppi', 'pura', 'purchas', 'pure', 'puriti', 'purpleu', 'purpos', 'purs', 'push', 'pushbutton', 'pussi', 'put', 'puttin', 'puzzel', 'puzzl', 'px3748', 'qatar', 'qatarrakhesh', 'qbank', 'qet', 'qi', 'qing', 'qlynnbv', 'qualiti', 'quarter', 'que', 'queen', 'queri', 'question', 'questionstd', 'quick', 'quickli', 'quiet', 'quit', 'quiteamuz', 'quiz', 'quizclub', 'quizwin', 'quizz', 'quot', 'r', 'r836', 'ra', 'racal', 'race', 'radiat', 'radio', 'rael', 'raglan', 'rahul', 'raiden', 'railway', 'rain', 'rais', 'raj', 'raja', 'rajini', 'rajipl', 'rajitha', 'rajnik', 'rakhesh', 'raksha', 'ralli', 'ralph', 'ramen', 'ran', 'randi', 'random', 'randomli', 'randomlli', 'rang', 'ranjith', 'ranju', 'rape', 'rat', 'rate', 'ratetc', 'rather', 'ratio', 'raviyog', 'rawr', 'ray', 'rayan', 'rayman', 'rcbbattl', 'rcd', 'rct', 'rcv', 'rcvd', 'rd', 'rdi', 're', 'reach', 'react', 'reaction', 'read', 'reader', 'readi', 'readyal', 'real', 'real1', 'reali', 'realis', 'realiti', 'realiz', 'realli', 'reallyne', 'reappli', 'rearrang', 'reason', 'reassur', 'rebel', 'reboot', 'rebtel', 'rec', 'recd', 'recdthirtyeight', 'receipt', 'receiv', 'receivea', 'recent', 'recept', 'recess', 'recharg', 'rechargerakhesh', 'reciev', 'reckon', 'recognis', 'record', 'recount', 'recoveri', 'recpt', 'recreat', 'recycl', 'red', 'redeem', 'redim', 'redr', 'reduc', 'ree', 'ref', 'ref9280114', 'ref9307622', 'refer', 'referin', 'reffer', 'refil', 'reflect', 'reflex', 'reformat', 'refresh', 'refund', 'refundedthi', 'refus', 'reg', 'regard', 'regist', 'registr', 'regret', 'regular', 'reject', 'rel', 'relat', 'relationshipit', 'relax', 'releas', 'reliant', 'reliev', 'religi', 'reloc', 'reltnship', 'rem', 'remain', 'remb', 'rememb', 'rememberi', 'remembr', 'remet', 'remind', 'remov', 'rencontr', 'renew', 'rent', 'rental', 'rentl', 'repair', 'repeat', 'repent', 'replac', 'repli', 'replyb', 'replys150', 'report', 'reppurcuss', 'repres', 'republ', 'request', 'requir', 'reschedul', 'research', 'resend', 'resent', 'reserv', 'reset', 'resid', 'resiz', 'reslov', 'resolut', 'resolv', 'resort', 'respect', 'responcewhat', 'respond', 'respons', 'rest', 'restaur', 'restock', 'restrict', 'restuwud', 'restwish', 'resub', 'resubmit', 'result', 'resum', 'retard', 'retir', 'retriev', 'return', 'reunion', 'reveal', 'revers', 'review', 'revis', 'reward', 'rg21', 'rgd', 'rgent', 'rhode', 'rhythm', 'rice', 'rich', 'riddanc', 'ridden', 'ride', 'right', 'rightio', 'rightli', 'riley', 'rimac', 'ring', 'ringsreturn', 'rington', 'ringtonefrom', 'ringtoneget', 'ringtonek', 'rinu', 'rip', 'rise', 'risk', 'rite', 'ritten', 'river', 'ro', 'road', 'roadsrvx', 'roast', 'rob', 'robinson', 'rock', 'rodds1', 'rodger', 'rofl', 'roger', 'role', 'roll', 'roller', 'romant', 'romcapspam', 'ron', 'room', 'roomat', 'roommat', 'rose', 'rough', 'round', 'rounderso', 'rout', 'row', 'roww1j6hl', 'roww1jhl', 'royal', 'rp176781', 'rpl', 'rpli', 'rr', 'rreveal', 'rs', 'rs5', 'rsi', 'rstm', 'rtking', 'rtm', 'rto', 'ru', 'rub', 'rubber', 'rude', 'rudi', 'rugbi', 'ruin', 'rule', 'rum', 'rumbl', 'rummer', 'rumour', 'run', 'runninglet', 'rupaul', 'rush', 'ryan', 'ryder', 's', 's3xi', 's89', 'sac', 'sachin', 'sachinjust', 'sack', 'sacrific', 'sad', 'sae', 'saeed', 'safe', 'safeti', 'sagamu', 'saibaba', 'said', 'saidif', 'sake', 'salad', 'salam', 'salari', 'sale', 'salesman', 'salespe', 'sall', 'salmon', 'salon', 'salt', 'sam', 'samachara', 'samantha', 'sambarlif', 'same', 'sameso', 'samu', 'sandiago', 'sane', 'sang', 'sankranti', 'santa', 'santha', 'sao', 'sapna', 'sar', 'sara', 'sarasota', 'sarcasm', 'sarcast', 'sari', 'saristar', 'sariyag', 'sashimi', 'sat', 'satan', 'sathi', 'sathya', 'satisfi', 'satjust', 'satlov', 'satsgettin', 'satsound', 'satthen', 'saturday', 'sat', 'sauci', 'sausagelov', 'savamob', 'save', 'saw', 'say', 'sayask', 'sayhey', 'sayi', 'sayin', 'sbut', 'sc', 'scalli', 'scammer', 'scarcasim', 'scare', 'scari', 'scenario', 'sceneri', 'sch', 'schedul', 'school', 'scienc', 'scold', 'scool', 'scorabl', 'score', 'scotch', 'scotland', 'scotsman', 'scous', 'scrape', 'scrappi', 'scratch', 'scream', 'screen', 'screwd', 'scroung', 'scrumptiou', 'sculptur', 'sd', 'sday', 'sdryb8i', 'se', 'sea', 'search', 'season', 'seat', 'sec', 'second', 'secondari', 'secret', 'secretari', 'secretli', 'section', 'secur', 'sed', 'see', 'seed', 'seek', 'seeker', 'seem', 'seen', 'seeno', 'sef', 'seh', 'sehwag', 'select', 'self', 'selfindepend', 'selfish', 'selfless', 'sell', 'sem', 'semest', 'semi', 'semiobscur', 'sen', 'send', 'sender', 'sendernam', 'senor', 'senrddnot', 'sens', 'sensesrespect', 'sensibl', 'sensit', 'sent', 'sentdat', 'sentenc', 'senthil', 'senthilhsbc', 'seperated', 'sept', 'septemb', 'serena', 'seri', 'seriou', 'serious', 'serv', 'server', 'servic', 'set', 'settl', 'seven', 'seventeen', 'sever', 'sex', 'sexi', 'sexiest', 'sextextukcom', 'sexual', 'sexychat', 'sez', 'sfine', 'sfirst', 'sfrom', 'sh', 'sha', 'shade', 'shadow', 'shag', 'shah', 'shahjahan', 'shakara', 'shake', 'shakespear', 'shall', 'shame', 'shampain', 'shangela', 'shanghai', 'shanilrakhesh', 'shant', 'shape', 'share', 'shatter', 'shave', 'shb', 'shd', 'she', 'sheet', 'sheffield', 'shelf', 'shell', 'shelv', 'sherawat', 'shesil', 'shexi', 'shhhhh', 'shi', 'shifad', 'shija', 'shijutta', 'shinco', 'shindig', 'shine', 'shini', 'ship', 'shirt', 'shit', 'shite', 'shitin', 'shitjustfound', 'shitload', 'shitstorm', 'shivratri', 'shja', 'shld', 'shldxxxx', 'shock', 'shoe', 'shola', 'shoot', 'shop', 'shoppin', 'shopth', 'shopw', 'shoranur', 'shore', 'shoreth', 'short', 'shortag', 'shortcod', 'shorter', 'shortli', 'shot', 'shoul', 'should', 'shoulder', 'shouldn', 'shouldnt', 'shout', 'shove', 'show', 'shower', 'showr', 'showroomsc', 'shracomorsglsuplt10', 'shrek', 'shrink', 'shrub', 'shu', 'shud', 'shudvetold', 'shuhui', 'shun', 'shut', 'si', 'sian', 'sib', 'sic', 'sick', 'sicomo', 'side', 'sif', 'sigh', 'sight', 'sign', 'signal', 'signific', 'signin', 'siguviri', 'silenc', 'silent', 'silli', 'silver', 'sim', 'simonwatson5120', 'simpl', 'simpler', 'simpli', 'simpson', 'simul', 'sinc', 'sinco', 'sindu', 'sing', 'singapor', 'singl', 'sink', 'sip', 'sipix', 'sir', 'siri', 'sirjii', 'sirsalam', 'sister', 'sit', 'site', 'sitll', 'sitter', 'sittin', 'situat', 'siva', 'sivatat', 'six', 'size', 'sk3', 'sk38xh', 'skalli', 'skateboard', 'ski', 'skilgm', 'skill', 'skillgam', 'skillgame1winaweek', 'skin', 'skinni', 'skint', 'skip', 'skirt', 'sky', 'skye', 'skype', 'skyve', 'slaaaaav', 'slack', 'slap', 'slave', 'sleep', 'sleepi', 'sleepin', 'sleepingand', 'sleepingwith', 'sleepsweet', 'sleepwellamptak', 'slept', 'slice', 'slide', 'slightli', 'slip', 'slipper', 'slipperi', 'slo', 'slo4msg', 'slob', 'slot', 'slove', 'slow', 'slower', 'slowli', 'slurp', 'sm', 'smack', 'small', 'smaller', 'smart', 'smartcal', 'smarter', 'smartthough', 'smash', 'smear', 'smell', 'smeon', 'smidgin', 'smile', 'smiley', 'smith', 'smithswitch', 'smoke', 'smokin', 'smoothli', 'sms08718727870', 'smsd', 'smsing', 'smsservic', 'smsshsexnetun', 'smth', 'sn', 'snake', 'snap', 'snappi', 'snatch', 'snd', 'sneham', 'snicker', 'sno', 'snog', 'snoringthey', 'snow', 'snowbal', 'snowboard', 'snowman', 'snuggl', 'so', 'soani', 'soc', 'socht', 'social', 'sofa', 'soft', 'softwar', 'soil', 'soire', 'sol', 'soladha', 'sold', 'solihul', 'solv', 'some', 'some1', 'somebodi', 'someday', 'someon', 'someonethat', 'someonon', 'someplac', 'somerset', 'someth', 'somethin', 'sometim', 'sometimerakheshvisitor', 'sometm', 'somewhat', 'somewher', 'somewheresomeon', 'somewhr', 'somon', 'somtim', 'sonathaya', 'sonetim', 'song', 'soni', 'sonot', 'sonyericsson', 'soo', 'soon', 'soonc', 'sooner', 'soonlot', 'soonxxx', 'sooo', 'soooo', 'sooooo', 'sopha', 'sore', 'sori', 'sorri', 'sorrow', 'sorrowsi', 'sorryi', 'sorryin', 'sort', 'sorta', 'sortedbut', 'sorydarealyfrm', 'soso', 'soul', 'sound', 'soundtrack', 'soup', 'sourc', 'south', 'southern', 'souveni', 'soz', 'space', 'spacebuck', 'spageddi', 'spain', 'spam', 'spanish', 'spare', 'spark', 'sparkl', 'spatula', 'speak', 'spec', 'special', 'specialcal', 'specialis', 'specif', 'specifi', 'speechless', 'speed', 'speedchat', 'spele', 'spell', 'spend', 'spent', 'spi', 'spice', 'spider', 'spiderman', 'spif', 'spile', 'spin', 'spinout', 'spiral', 'spirit', 'spiritu', 'spjanuari', 'spk', 'spl', 'splash', 'splashmobil', 'splat', 'splendid', 'split', 'splle', 'splwat', 'spoil', 'spoilt', 'spoke', 'spoken', 'sponsor', 'spontan', 'spook', 'spoon', 'sporad', 'sport', 'sportsx', 'spose', 'spot', 'spotti', 'spous', 'sppok', 'spreadsheet', 'spree', 'spring', 'sprint', 'sprwm', 'sptv', 'sptyron', 'spunout', 'sq825', 'squat', 'squeeeeez', 'squeez', 'squid', 'squishi', 'sr', 'sri', 'srsli', 'srt', 'ssi', 'ssindia', 'ssnervou', 'st', 'stabil', 'stabl', 'stadium', 'staff', 'staffsciencenusedusgphyhcmkteachingpc1323', 'stage', 'stagwood', 'stair', 'stalk', 'stamp', 'stand', 'standard', 'stapati', 'star', 'stare', 'starer', 'starshin', 'start', 'startedindia', 'starti', 'starv', 'starwars3', 'stash', 'state', 'statement', 'station', 'statu', 'stay', 'stayin', 'std', 'stdtxtrate', 'steak', 'steal', 'steam', 'steamboat', 'steed', 'steer', 'step', 'stereo', 'stereophon', 'sterl', 'sterm', 'steve', 'stevelik', 'stewarts', 'steyn', 'sth', 'sthi', 'stick', 'sticki', 'stifl', 'stil', 'still', 'stillmayb', 'stink', 'stitch', 'stock', 'stockport', 'stolen', 'stomach', 'stomp', 'stone', 'stoner', 'stool', 'stop', 'stop2', 'stop2stop', 'stopbcm', 'stopc', 'stopcost', 'stoptxt', 'stoptxtstop', 'store', 'storelik', 'stori', 'storm', 'str', 'str8', 'straight', 'strain', 'strang', 'stranger', 'strangersaw', 'stream', 'street', 'streetshal', 'stress', 'stressful', 'stretch', 'strewn', 'strict', 'strike', 'string', 'strip', 'stripe', 'stroke', 'strong', 'strongbuy', 'strongli', 'strt', 'strtd', 'struggl', 'stu', 'stubborn', 'stuck', 'studdi', 'student', 'studentfinanci', 'studentsthi', 'studi', 'studio', 'studyn', 'stuf', 'stuff', 'stuff42moro', 'stuffleav', 'stuffwhi', 'stun', 'stupid', 'stupidit', 'style', 'stylish', 'stylist', 'sub', 'subject', 'sublet', 'submit', 'subpoli', 'subscrib', 'subscribe6gbpmnth', 'subscript', 'subscriptn3gbpwk', 'subscrit', 'subsequ', 'subtoitl', 'success', 'such', 'suck', 'sucker', 'sudden', 'suddenli', 'sudn', 'sue', 'suffer', 'suffici', 'sugabab', 'suganya', 'sugar', 'sugardad', 'suggest', 'suit', 'suitem', 'sullivan', 'sum', 'sum1', 'sumf', 'summer', 'summon', 'sumthin', 'sumthinxx', 'sun', 'sun0819', 'sunday', 'sundayish', 'sunlight', 'sunni', 'sunoco', 'sunroof', 'sunscreen', 'sunshin', 'suntec', 'sup', 'super', 'superb', 'superior', 'supervisor', 'supli', 'supos', 'suppli', 'supplier', 'support', 'supportprovid', 'supportveri', 'suppos', 'suprem', 'suprman', 'sura', 'sure', 'surf', 'surgic', 'surli', 'surnam', 'surpris', 'surrend', 'surround', 'survey', 'surya', 'sutra', 'sux', 'suzi', 'svc', 'sw7', 'sw73ss', 'swalpa', 'swan', 'swann', 'swap', 'swashbuckl', 'swat', 'swatch', 'sway', 'swayz', 'swear', 'sweater', 'sweatter', 'sweet', 'sweetest', 'sweetheart', 'sweeti', 'swell', 'swhrt', 'swim', 'swimsuit', 'swing', 'swiss', 'switch', 'swollen', 'swoop', 'swt', 'swtheart', 'syd', 'syllabu', 'symbol', 'sympathet', 'symptom', 'sync', 'syria', 'syrup', 'system', 't', 't91', 'ta', 'tabl', 'tablet', 'tackl', 'taco', 'tact', 'tactless', 'tadaaaaa', 'tag', 'tahan', 'tai', 'tait', 'taj', 'taka', 'take', 'takecar', 'taken', 'takenonli', 'takin', 'talent', 'talk', 'talkbut', 'talkin', 'tall', 'tallahasse', 'tallent', 'tamilnaduthen', 'tampa', 'tank', 'tantrum', 'tap', 'tape', 'tariff', 'tarot', 'tarpon', 'tast', 'tat', 'tata', 'tattoo', 'tau', 'taught', 'taunton', 'tax', 'taxi', 'taxless', 'taxt', 'taylor', 'tayseertissco', 'tb', 'tbspersolvo', 'tc', 'tcllc', 'tcrw1', 'tcsbcm4235wc1n3xx', 'tcsc', 'tcsstop', 'tddnewsletteremc1couk', 'tea', 'teach', 'teacher', 'teacoffe', 'team', 'tear', 'teas', 'tech', 'technic', 'technolog', 'tee', 'teenag', 'teeth', 'teethi', 'teethif', 'teju', 'tel', 'telephon', 'teletext', 'tell', 'telli', 'tellmiss', 'telphon', 'telugu', 'telugutht', 'temal', 'temp', 'temper', 'templ', 'ten', 'tenant', 'tendenc', 'tenerif', 'tens', 'tension', 'teresa', 'term', 'terminatedw', 'termsappli', 'terri', 'terribl', 'terrif', 'terrorist', 'tesco', 'tessypl', 'test', 'tex', 'texa', 'texd', 'text', 'text82228', 'textand', 'textbook', 'textbuddi', 'textcomp', 'textin', 'textoper', 'textpod', 'textsweekend', 'tgxxrz', 'th', 'than', 'thandiyachu', 'thangam', 'thangamit', 'thank', 'thanks2', 'thanksgiv', 'thanku', 'thankyou', 'thanx', 'thanx4', 'thanxxx', 'thasa', 'that', 'that2worzel', 'thatd', 'thatdont', 'thati', 'thatll', 'thatmum', 'thatnow', 'the', 'the4th', 'theacus', 'theater', 'theatr', 'thecd', 'thedailydraw', 'their', 'thekingshead', 'them', 'theme', 'themob', 'themobhit', 'themobyo', 'themp', 'then', 'thenwil', 'theoret', 'theori', 'theplac', 'thepub', 'there', 'theredo', 'theregoodnight', 'therel', 'therer', 'therexx', 'these', 'theseday', 'theseyour', 'thesi', 'thesmszonecom', 'thewend', 'they', 'theyll', 'theyr', 'thgt', 'thi', 'thia', 'thin', 'thing', 'thinghow', 'think', 'thinkin', 'thinkthi', 'thinl', 'thirunelvali', 'thisdon', 'thk', 'thkin', 'thm', 'thnk', 'thnq', 'thnx', 'tho', 'those', 'thoso', 'thot', 'thou', 'though', 'thought', 'thoughtsi', 'thousand', 'thout', 'thread', 'threat', 'three', 'threw', 'thriller', 'throat', 'through', 'throw', 'throwin', 'thrown', 'thru', 'thrurespect', 'tht', 'thu', 'thuglyf', 'thur', 'thursday', 'thx', 'ti', 'tick', 'ticket', 'tiempo', 'tiger', 'tight', 'tightli', 'tigress', 'tih', 'tiim', 'til', 'till', 'tim', 'time', 'timedhoni', 'timegud', 'timehop', 'timeslil', 'timey', 'timeyour', 'timi', 'timin', 'tini', 'tip', 'tire', 'tirunelvai', 'tirunelvali', 'tirupur', 'tisscotays', 'titl', 'titleso', 'tiwari', 'tix', 'tiz', 'tke', 'tkt', 'tlk', 'tm', 'tming', 'tmobil', 'tmorrowpl', 'tmr', 'tmrw', 'tmw', 'tnc', 'to', 'toa', 'toaday', 'tobacco', 'tobe', 'tocallshal', 'toclaim', 'today', 'todaybut', 'todaydo', 'todayfrom', 'todaygood', 'todayh', 'todaysundaysunday', 'todo', 'tog', 'togeth', 'tohar', 'toilet', 'tok', 'toke', 'token', 'tol', 'told', 'toldsh', 'toledo', 'toler', 'toleratbc', 'toll', 'tom', 'tomarrow', 'tome', 'tomeandsaidthi', 'tomo', 'tomoc', 'tomorro', 'tomorrow', 'tomorrowcal', 'tomorrowtoday', 'tomorw', 'ton', 'tone', 'tones2u', 'tones2youcouk', 'tonesrepli', 'tonex', 'tonght', 'tongu', 'tonight', 'tonit', 'tonitebusi', 'toniteth', 'tonsolitusaswel', 'too', 'took', 'tookplac', 'tool', 'toolet', 'tooo', 'toopray', 'toot', 'toothpast', 'tootsi', 'top', 'topic', 'topicsorri', 'toplay', 'toppoli', 'tor', 'torch', 'torrent', 'tortilla', 'tortur', 'tosend', 'toshiba', 'toss', 'tot', 'total', 'tote', 'touch', 'tough', 'toughest', 'tour', 'toward', 'town', 'towncud', 'towndontmatt', 'toxic', 'toyota', 'tp', 'track', 'trackmarqu', 'trade', 'tradit', 'traffic', 'train', 'trainner', 'tram', 'tranquil', 'transact', 'transcrib', 'transfer', 'transferacc', 'transfr', 'transport', 'trash', 'trauma', 'trav', 'travel', 'treacl', 'treadmil', 'treasur', 'treat', 'treatin', 'trebl', 'tree', 'trek', 'trend', 'tri', 'trial', 'trip', 'tripl', 'trishul', 'triumph', 'tron', 'troubl', 'troubleshoot', 'trouser', 'trubl', 'truck', 'true', 'truekdo', 'truffl', 'truli', 'truro', 'trust', 'truth', 'tryin', 'trywal', 'ts', 'tsandc', 'tsc', 'tscs08714740323', 'tscs087147403231winawkage16', 'tshirt', 'tsunami', 'tt', 'ttyl', 'tue', 'tuesday', 'tui', 'tuition', 'tul', 'tulip', 'tund', 'tune', 'tunji', 'turkey', 'turn', 'tuth', 'tv', 'tvhe', 'tvlol', 'twat', 'twelv', 'twenti', 'twice', 'twigg', 'twilight', 'twin', 'twink', 'twitter', 'two', 'txt', 'txt250com', 'txtauction', 'txtauctiontxt', 'txtin', 'txting', 'txtjourney', 'txtno', 'txtx', 'tyler', 'type', 'typelyk', 'typic', 'u', 'u2moro', 'uawakefeellikw', 'ubandu', 'ubi', 'ucal', 'ufind', 'ugadi', 'ugh', 'ugo', 'uh', 'uhhhhrmm', 'uif', 'uin', 'ujhhhhhhh', 'uk', 'ukmobiled', 'ukp2000', 'ull', 'ultim', 'ultimatum', 'um', 'umma', 'ummmawil', 'ummmmmaah', 'un', 'unabl', 'unbeliev', 'uncl', 'unclaim', 'uncomfort', 'uncondit', 'unconsci', 'unconvinc', 'uncount', 'uncut', 'under', 'underdtand', 'understand', 'understood', 'underwear', 'undrstnd', 'undrstndng', 'unemploy', 'unev', 'unfold', 'unfortun', 'unfortuntli', 'unhappi', 'uni', 'unicef', 'uniform', 'unintent', 'uniqu', 'uniquei', 'unit', 'univ', 'univers', 'unknown', 'unless', 'unlik', 'unlimit', 'unmit', 'unnecessarili', 'unni', 'unrecogn', 'unredeem', 'unsecur', 'unsold', 'unsoldmik', 'unsoldnow', 'unspoken', 'unsub', 'unsubscrib', 'until', 'unusu', 'uothrwis', 'up', 'up4', 'upcharg', 'upd8', 'updat', 'updatenow', 'upgrad', 'upgrdcentr', 'uphad', 'upload', 'upnot', 'upon', 'upset', 'upseti', 'upsetit', 'upstair', 'upto', 'uptown', 'upyeh', 'ur', 'ure', 'urfeel', 'urgent', 'urgentbut', 'urgentlyit', 'urgh', 'urgnt', 'urgoin', 'urgran', 'urin', 'url', 'urmomi', 'urn', 'urself', 'us', 'usb', 'usc', 'uscedu', 'use', 'useless', 'user', 'usf', 'usget', 'usher', 'uslet', 'usml', 'usno', 'uso', 'usp', 'usual', 'usualiam', 'uteru', 'utter', 'uup', 'uv', 'uve', 'uwana', 'uwant', 'uworld', 'uxxxx', 'v', 'vaazhthukk', 'vagu', 'vai', 'vale', 'valentin', 'valid', 'valid12hr', 'valu', 'valuabl', 'valuemorn', 'varaya', 'vargu', 'vari', 'variou', 'varma', 'vasai', 'vat', 'vatian', 'vava', 'vco', 'vday', 've', 'vega', 'veget', 'veggi', 'vehicl', 'velacheri', 'velli', 'velusami', 'venaam', 'venugop', 'veri', 'verifi', 'version', 'versu', 'vettam', 'vewi', 'via', 'vibrant', 'vibrat', 'vic', 'victor', 'victoria', 'vid', 'video', 'videochat', 'videop', 'videophon', 'videosound', 'videosounds2', 'vidnot', 'view', 'vijay', 'vijaykanth', 'vikki', 'vikkyim', 'vilikkamt', 'vill', 'villa', 'villag', 'vinobanagar', 'violat', 'violenc', 'violet', 'vip', 'virgil', 'virgin', 'virtual', 'visa', 'visionsmscom', 'visit', 'visitne', 'visitor', 'vital', 'vitamin', 'viva', 'vivek', 'vivekanand', 'viveki', 'vl', 'vldo', 'voda', 'vodafon', 'vodka', 'voic', 'voicemail', 'voila', 'volcano', 'vomit', 'vomitin', 'vote', 'voucher', 'voucherstext', 'vpist', 'vpod', 'vri', 'vs', 'vth', 'vtire', 'w', 'w111wx', 'w14rg', 'w1a', 'w1j', 'w1t1ji', 'w45wq', 'w8in', 'wa', 'wa14', 'waaaat', 'wad', 'wadebridgei', 'wah', 'wahala', 'wahay', 'wahe', 'waheeda', 'wahleykkumshar', 'waht', 'wait', 'waiti', 'waitin', 'waitshould', 'waitu', 'wake', 'wale', 'walik', 'walk', 'walkabout', 'walkin', 'wall', 'wallet', 'wallpap', 'wallpaperal', 'walmart', 'walsal', 'wamma', 'wan', 'wan2', 'wana', 'wanna', 'wannatel', 'want', 'want2com', 'wap', 'waqt', 'warm', 'warn', 'warner', 'warranti', 'warwick', 'washob', 'wasn', 'wasnt', 'wast', 'wat', 'watch', 'watchin', 'watchng', 'wate', 'water', 'watev', 'watevr', 'watll', 'watrdayno', 'watt', 'wave', 'way', 'way2smscom', 'waythi', 'wc', 'wc1n', 'wc1n3xx', 'we', 'weak', 'weapon', 'wear', 'weasel', 'weather', 'web', 'web2mobil', 'webadr', 'webeburnin', 'webpag', 'websit', 'websitenow', 'wed', 'weddin', 'weddingfriend', 'wedlunch', 'wednesday', 'wee', 'weed', 'weeddefici', 'week', 'weekday', 'weekend', 'weekli', 'weekstop', 'weigh', 'weight', 'weighthaha', 'weightloss', 'weird', 'weirdest', 'weirdi', 'weirdo', 'weiyi', 'welcom', 'well', 'wellda', 'welli', 'welltak', 'wellyou', 'welp', 'wen', 'wendi', 'wenev', 'went', 'wenwecan', 'wer', 'were', 'werear', 'werebor', 'werent', 'wereth', 'wesley', 'west', 'western', 'westlif', 'westonzoyland', 'westshor', 'wet', 'wetherspoon', 'weve', 'wewa', 'whassup', 'what', 'whatev', 'whatsup', 'wheat', 'wheel', 'wheellock', 'when', 'whenev', 'whenevr', 'whenr', 'whenwher', 'where', 'wherear', 'wherebtw', 'wherev', 'wherevr', 'wherr', 'whether', 'whi', 'which', 'while', 'whileamp', 'whilltak', 'whisper', 'white', 'whn', 'who', 'whole', 'whom', 'whore', 'whose', 'whr', 'wi', 'wick', 'wicket', 'wicklow', 'wid', 'widelivecomindex', 'wif', 'wife', 'wifedont', 'wifehow', 'wifi', 'wihtuot', 'wikipediacom', 'wil', 'wild', 'wildest', 'wildlif', 'will', 'willpow', 'win', 'win150ppmx3age16', 'wind', 'windi', 'window', 'wine', 'wing', 'winner', 'winnersclub', 'winterston', 'wipe', 'wipro', 'wiproy', 'wire3net', 'wisdom', 'wise', 'wish', 'wishin', 'wishlist', 'wiskey', 'wit', 'with', 'withdraw', 'wither', 'within', 'without', 'witin', 'witot', 'witout', 'wiv', 'wizzl', 'wk', 'wkend', 'wkent150p16', 'wkg', 'wkli', 'wknd', 'wktxt', 'wlcome', 'wld', 'wmlid1b6a5ecef91ff937819firsttrue180430jul05', 'wmlid820554ad0a1705572711firsttru', 'wnevr', 'wnt', 'wo', 'woah', 'wocay', 'woke', 'woken', 'woman', 'womdarful', 'women', 'won', 'wondar', 'wondarful', 'wonder', 'wont', 'woo', 'wood', 'woodland', 'woohoo', 'woot', 'woould', 'woozl', 'worc', 'word', 'wordcollect', 'wordnot', 'wordsevri', 'wordstart', 'work', 'workag', 'workand', 'workin', 'worklov', 'workout', 'world', 'worldgnun', 'worldmay', 'worldveri', 'worm', 'worri', 'worriedx', 'worryc', 'worryus', 'wors', 'worst', 'worth', 'worthless', 'wot', 'wotu', 'wotz', 'woul', 'would', 'woulda', 'wouldnt', 'wound', 'wow', 'wquestion', 'wrc', 'wreck', 'wrench', 'wright', 'write', 'writh', 'wrk', 'wrki', 'wrkin', 'wrking', 'wrld', 'wrnog', 'wrong', 'wrongli', 'wrongtak', 'wrote', 'ws', 'wt', 'wtc', 'wtf', 'wth', 'wthout', 'wud', 'wudnt', 'wuld', 'wuldnt', 'wun', 'www07781482378com', 'www4tcbiz', 'www80488biz', 'wwwapplausestorecom', 'wwwareyouuniquecouk', 'wwwasjesuscom', 'wwwb4utelecom', 'wwwbridalpetticoatdreamscouk', 'wwwcashbincouk', 'wwwclubmobycom', 'wwwclubzedcouk', 'wwwcnupdatescomnewslett', 'wwwcomuknet', 'wwwdbuknet', 'wwwflirtpartyu', 'wwwfullonsmscom', 'wwwgambtv', 'wwwgetzedcouk', 'wwwidewcom', 'wwwldewcom', 'wwwldewcom1win150ppmx3age16', 'wwwldewcom1win150ppmx3age16subscript', 'wwwldewcomsubs161win150ppmx3', 'wwwmovietriviatv', 'wwwmusictrivianet', 'wwworangecoukow', 'wwwphb1com', 'wwwregalportfoliocouk', 'wwwringtonekingcouk', 'wwwringtonescouk', 'wwwrtfsphostingcom', 'wwwsantacallingcom', 'wwwshortbreaksorguk', 'wwwsmsacubootydeli', 'wwwsmsacugoldvik', 'wwwsmsacuhmmross', 'wwwsmsacunat27081980', 'wwwsmsacunatalie2k9', 'wwwsmsconet', 'wwwtcbiz', 'wwwtelediscountcouk', 'wwwtextcompcom', 'wwwtextpodnet', 'wwwtklscom', 'wwwtxt2shopcom', 'wwwtxt43com', 'wwwtxt82228com', 'wwwtxttowincouk', 'wwwwin82050couk', 'wyli', 'x', 'x29', 'x49', 'x49your', 'xafter', 'xam', 'xavier', 'xchat', 'xclusiveclubsaisai', 'xin', 'xma', 'xnet', 'xoxo', 'xt', 'xuhui', 'xx', 'xxsp', 'xxuk', 'xxx', 'xxxmobilemovieclub', 'xxxmobilemovieclubcomnqjkgighjjgcbl', 'xxxx', 'xxxxx', 'xxxxxx', 'xxxxxxx', 'xxxxxxxx', 'xxxxxxxxxxxxxx', 'xy', 'y', 'y87', 'ya', 'yago', 'yah', 'yahoo', 'yalrigu', 'yalru', 'yam', 'yan', 'yar', 'yard', 'yavnt', 'yaxx', 'yaxxx', 'yay', 'yck', 'yday', 'ye', 'yeah', 'yeahand', 'year', 'yeesh', 'yeh', 'yell', 'yellow', 'yelowi', 'yen', 'yeovil', 'yep', 'yer', 'yes165', 'yes434', 'yes440', 'yes762', 'yes910', 'yesbut', 'yesfrom', 'yesgauti', 'yesh', 'yesher', 'yesim', 'yesmum', 'yessura', 'yest', 'yesterday', 'yet', 'yetti', 'yetund', 'yi', 'yifeng', 'yiju', 'yijuehotmailcom', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yoher', 'yor', 'yorg', 'you', 'youani', 'youcarlo', 'youclean', 'youd', 'youdearwith', 'youdo', 'youhow', 'youi', 'youkwher', 'yould', 'youll', 'youmi', 'youmoney', 'young', 'younger', 'youphon', 'your', 'yourinclus', 'yourjob', 'yourself', 'youso', 'youthat', 'youto', 'youuuuu', 'youv', 'youwanna', 'youwhen', 'yovil', 'yowif', 'yoyyooo', 'yr', 'ystrdayic', 'yummi', 'yummmm', 'yun', 'yunni', 'yuo', 'yuou', 'yup', 'yupz', 'ywhere', 'zac', 'zaher', 'zealand', 'zebra', 'zed', 'zero', 'zhong', 'zindgi', 'zoe', 'zogtoriu', 'zoom', 'zouk', 'zyada', '', '', '', 'll', 'ud']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "print(X_counts.shape)\n",
    "print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Apply CountVectorizer to smaller sample for sake of understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 266)\n",
      "['08002986030', '08452810075over18', '09061701461', '1', '100', '100000', '11', '12', '150', '150pday', '16', '2', '20000', '2005', '21st', '3', '4', '4403ldnw1a7rw18', '4txt120', '6day', '81010', '87077', '87121', '87575', '9', '900', 'A', 'As', 'Co', 'Eh', 'FA', 'HL', 'He', 'I', 'Id', 'Im', 'Is', 'No', 'ON', 'Oh', 'Ok', 'R', 'TC', 'Tb', 'To', 'U', 'a', 'about', 'aid', 'all', 'alreadi', 'and', 'anymor', 'appli', 'around', 'as', 'at', 'b', 'back', 'be', 'been', 'bless', 'breather', 'brother', 'c', 'call', 'caller', 'callertun', 'camera', 'cash', 'chanc', 'chg', 'claim', 'click', 'code', 'colour', 'comp', 'copi', 'cost', 'credit', 'cri', 'csh11', 'cup', 'custom', 'darl', 'date', 'did', 'dont', 'dun', 'earli', 'eg', 'england', 'enough', 'entitl', 'entri', 'even', 'feel', 'final', 'fine', 'for', 'free', 'freemsg', 'friend', 'from', 'fulfil', 'fun', 'goalsteam', 'goe', 'gonna', 'gota', 'grant', 'ha', 'had', 'have', 'he', 'help', 'here', 'hey', 'hi', 'home', 'hor', 'hour', 'how', 'httpwap', 'i', 'if', 'in', 'info', 'is', 'it', 'ive', 'jackpot', 'joke', 'k', 'kim', 'kl341', 'lar', 'latest', 'lccltd', 'like', 'link', 'live', 'macedonia', 'make', 'may', 'me', 'mell', 'membership', 'messag', 'minnaminungint', 'miss', 'mobil', 'month', 'more', 'my', 'nah', 'name', 'nation', 'naughti', 'network', 'news', 'next', 'no', 'not', 'now', 'nurungu', 'ok', 'on', 'oni', 'onli', 'or', 'oru', 'our', 'patent', 'per', 'pobox', 'poboxox36504w45wq', 'pound', 'press', 'prize', 'promis', 'questionstd', 'ratetc', 'rcv', 'receiv', 'receivea', 'rememb', 'repli', 'request', 'reward', 'right', 's', 'say', 'scotland', 'search', 'select', 'send', 'serious', 'set', 'six', 'so', 'some', 'soon', 'speak', 'spell', 'std', 'still', 'stuff', 'sunday', 'take', 'talk', 'team', 'text', 'thank', 'that', 'the', 'then', 'there', 'they', 'thi', 'think', 'though', 'time', 'tkt', 'to', 'today', 'tonight', 'treat', 'trywal', 'tsandc', 'txt', 'u', 'until', 'up', 'updat', 'ur', 'urgent', 'use', 'usf', 'v', 'valid', 'valu', 'vettam', 'want', 'wap', 'watch', 'way', 'week', 'wet', 'wif', 'will', 'win', 'winner', 'with', 'wkli', 'won', 'wonder', 'wont', 'word', 'wwwdbuknet', 'xxx', 'xxxmobilemovieclub', 'xxxmobilemovieclubcomnqjkgighjjgcbl', 'ye', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "data_sample = data[0:20]\n",
    "\n",
    "count_vect_sample = CountVectorizer(analyzer=clean_text)\n",
    "X_counts_sample = count_vect_sample.fit_transform(data_sample['body_text'])\n",
    "\n",
    "print(X_counts_sample.shape)\n",
    "print(count_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizers output sparse metrices\n",
    "\n",
    "Sparse Matrix: \n",
    "    A matrix in which most entried are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x266 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 352 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( X_counts_sample.toarray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
